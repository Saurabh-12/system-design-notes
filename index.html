<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta name="google-site-verification" content="GBCPEM6p0IarruRZ2_Pu_sz_QCvsjaQL87hWnv7zcj0" />
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <!-- Begin Jekyll SEO tag v2.8.0 -->
    <title>System Design Notes | system-design-notes</title>
    <meta name="generator" content="Jekyll v3.9.5" />
    <meta property="og:title" content="System Design Notes" />
    <meta property="og:locale" content="en_US" />
    <meta
      name="description"
      content="Markdown version of my system design notes, compiled from a bunch of different online resources"
    />
    <meta
      property="og:description"
      content="Markdown version of my system design notes, compiled from a bunch of different online resources"
    />
    <link rel="canonical" href="https://david8zhang.github.io/system-design-notes/" />
    <meta property="og:url" content="https://david8zhang.github.io/system-design-notes/" />
    <meta property="og:site_name" content="system-design-notes" />
    <meta property="og:type" content="website" />
    <meta name="twitter:card" content="summary" />
    <meta property="twitter:title" content="System Design Notes" />
    <script type="application/ld+json">
      {
        "@context": "https://schema.org",
        "@type": "WebSite",
        "description": "Markdown version of my system design notes, compiled from a bunch of different online resources",
        "headline": "System Design Notes",
        "name": "system-design-notes",
        "url": "https://david8zhang.github.io/system-design-notes/"
      }
    </script>
    <!-- End Jekyll SEO tag -->

    <link
      rel="stylesheet"
      href="/system-design-notes/assets/css/style.css?v=78935b962d47f4fc2bfefd462d3ca86cc35b2103"
    />
    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

    <!-- Setup Google Analytics -->

    <!-- You can set your favicon here -->
    <!-- link rel="shortcut icon" type="image/x-icon" href="/system-design-notes/favicon.ico" -->

    <!-- end custom head snippets -->
  </head>
  <body>
    <div class="container-lg px-3 my-5 markdown-body">
      <h1><a href="https://david8zhang.github.io/system-design-notes/">system-design-notes</a></h1>

      <h1 id="system-design-notes">System Design Notes</h1>

      <p>
        These are all the notes I’ve taken while studying for system design interivews. The top
        resources I’ve used are:
      </p>

      <ul>
        <li>
          <a
            href="https://www.amazon.com/Designing-Data-Intensive-Applications-Reliable-Maintainable/dp/1449373321/ref=sr_1_1?keywords=designing+data+intensive+applications&amp;link_code=qs&amp;qid=1705798730&amp;sr=8-1"
            ><em>Designing Data Intensive Applications</em></a
          >, by Martin Kleppmann
        </li>
        <li>
          <a href="https://www.youtube.com/playlist?list=PLjTveVh7FakLdTmm42TMxbN8PvVn5g4KJ"
            >jordanhasnolife’s System Design 2.0 Youtube channel</a
          >
        </li>
        <li>
          <a
            href="https://www.youtube.com/watch?v=lX4CrbXMsNQ&amp;list=PLCRMIe5FDPsd0gVs500xeOewfySTsmEjf"
            >ByteByteGo’s System Design Fundamentals Youtube Channel</a
          >
        </li>
        <li>
          Donne Martin’s
          <a href="https://github.com/donnemartin/system-design-primer">System Design Primer</a> on
          Github
        </li>
      </ul>

      <p>
        But I also reference other resources and articles as well, which I link in an “Additional
        Reading” section at the end of each topic module.
      </p>

      <p>
        I’m currently working on turning these notes into an interactive study tool called
        <strong>System Design Daily</strong>. You can check that out
        <a href="https://systemdesigndaily.com">here</a>
      </p>

      <p>Feel free to open a pull request if there are any inaccuracies in the content</p>

      <h1 id="storage--serialization">Storage &amp; Serialization</h1>

      <p>
        In this section, we’ll take a look at some formats for storing data, examining the pros and
        cons of each. This will include topics like relational data models, column compression, and
        SQL v.s NoSQL databases. We’ll also look at some frameworks for encoding the data to be sent
        over the network, like JSON, XML, Protobuf, and Avro
      </p>

      <h2 id="relationalnon-relational-data">Relational/Non-Relational Data</h2>

      <p>
        Relational data, or the relational model for representing data, is an intuitive,
        straightforward way of representing data in tables. Each table in our relational database
        only represents one type of data model. Relationships between tables are represented using
        <em>foriegn IDs</em>, which map two rows in two tables together. For example:
      </p>

      <p>We have a companies table representing our companies</p>

      <table>
        <thead>
          <tr>
            <th>Id</th>
            <th>Company</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>1</td>
            <td>Company A</td>
          </tr>
          <tr>
            <td>2</td>
            <td>Company B</td>
          </tr>
          <tr>
            <td>3</td>
            <td>Company C</td>
          </tr>
        </tbody>
      </table>

      <p>We have an cities table representing cities</p>

      <table>
        <thead>
          <tr>
            <th>Id</th>
            <th>City Name</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>1</td>
            <td>San Francisco</td>
          </tr>
          <tr>
            <td>2</td>
            <td>Seattle</td>
          </tr>
          <tr>
            <td>3</td>
            <td>New York</td>
          </tr>
        </tbody>
      </table>

      <p>
        A company offices table could be the result of joining the two tables, which tells us which
        companies have offices in which cities.
      </p>

      <table>
        <thead>
          <tr>
            <th>CompanyId</th>
            <th>CityId</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>1</td>
            <td>1</td>
          </tr>
          <tr>
            <td>1</td>
            <td>3</td>
          </tr>
          <tr>
            <td>2</td>
            <td>2</td>
          </tr>
          <tr>
            <td>2</td>
            <td>3</td>
          </tr>
          <tr>
            <td>3</td>
            <td>1</td>
          </tr>
        </tbody>
      </table>

      <p>
        Relational data is sometimes referred to as “normalized” data. A
        <strong>relational database</strong> is a type of database that stores this data, and
        typically uses SQL (Structured Query Langage) for querying and updating.
      </p>

      <h4 id="structured-query-language">Structured Query Language</h4>

      <p>
        As mentioned before, SQL is a programming language for storing and processing information in
        a relational database. It’s declarative, meaning we specify the expected result and core
        logic without directing the program’s control flow. Imperative, on the other hand, directs
        the control flow of the program. In other words, in declarative programming “you say what
        you want”, whereas in imperative programming you “say how to get what you want”.
      </p>

      <p>
        Declarative languages are good for database operations because they abstract away the
        underlying database implementation, enabling the system to make performance improvements
        without breaking queries. Furthermore, declarative languages lend themselves well to
        parallel execution, since they only specify the pattern of results and not the method used
        to determine them. Unlike with imperative code, the order of operations doesn’t matter.
      </p>

      <p>
        In practice, SQL statements can be executed in a specific way to maximize cache hits and
        ensure good performance. Many database systems have query optimizers which do these
        reorderings automatically behind the scenes.
      </p>

      <h4 id="disadvantages-of-relational-data">Disadvantages of Relational Data</h4>

      <p>
        Relational database tables in a single node might not be stored near each other on disk
        (poor data <em>locality</em>). That means trying to do the join across two tables could be
        slow due to random I/O on disk. In a distributed system, these tables might not even live on
        the same database node due to <em>partitioning</em> (which we’ll get into later). This would
        require us to make multiple network requests to different places, among other problems
        related to data consistency.
      </p>

      <p>
        Another issue that arises with relational data stems from the fact that many programming
        languages are object-oriented, meaning applications interact with data classes and objects.
        Relational data, with tables and rows, might not necessarily translate well - this issue is
        called <em>Object-relational Impedance Mismatch</em>. The most common way to mitigate this
        is through the use of Object-Relational Mappers (ORMs), which do exactly as their name
        implies - they translate objects to relational data and vice versa.
      </p>

      <h3 id="non-relational-data">Non-Relational Data</h3>

      <p>
        Nonrelational data uses a <em>denormalized</em> data model. For example, we could represent
        the same “company offices” relation above as a dictionary:
      </p>

      <div class="language-plaintext highlighter-rouge">
        <div class="highlight">
          <pre class="highlight"><code>{
  "Company A": ["San Francisco", "New York"],
  "Company B": ["Seattle", "New York"],
  "Company C": ["San Francisco"]
}
</code></pre>
        </div>
      </div>

      <p>
        Now, we have better data locality since we don’t have to query a company table and cities
        table separately to get the joined company offices results, everything we need is contained
        right there in the dictionary.
      </p>

      <p>
        However, this means that we have repeated data keys (“San Francisco”, and “New York”). Not
        only does this mean we need to store more data, this also means modifying our data could
        potentially be more complicated. If we wanted to remove “New York” from our list of cities,
        we’d need to update our data in multiple places.
      </p>

      <h3 id="relational-vs-non-relational">Relational vs. Non-Relational?</h3>

      <p>
        In general, we want to use non-relational data when all of our data is disjoint. For example
        if we have posts on Facebook, they’re typically not related to each other, and can be
        represented in a denormalized fashion. However, if we need to represent data types that
        might be related, such as which authors wrote certain books, we might be better served going
        with a relational database.
      </p>

      <h2 id="column-oriented-storage">Column Oriented Storage</h2>

      <p>
        <em>Row-oriented storage</em> has data for a single row stored together, which is basically
        like your regular relational database table. For example, the Employees table stored as
        <strong>employees.txt</strong>:
      </p>

      <table>
        <thead>
          <tr>
            <th>Name</th>
            <th>Email</th>
            <th>Company</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Alice</td>
            <td>alice@email.com</td>
            <td>Google</td>
          </tr>
          <tr>
            <td>Bob</td>
            <td>bob@email.com</td>
            <td>Amazon</td>
          </tr>
          <tr>
            <td>Charlie</td>
            <td>charlie@email.com</td>
            <td>Facebook</td>
          </tr>
        </tbody>
      </table>

      <p>
        <em>Column oriented storage</em> stores a bunch of column values together. So rather than
        having the Name, Email, and Company all in the same file, we split out each column into its
        own file and just store the value of that column for each row. For example we’d have a
        <strong>companies.txt</strong>, <strong>emails.txt</strong>, and <strong>names.txt</strong>:
      </p>

      <table>
        <thead>
          <tr>
            <th>Company</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Google</td>
          </tr>
          <tr>
            <td>Amazon</td>
          </tr>
          <tr>
            <td>Facebook</td>
          </tr>
          <tr>
            <td>…</td>
          </tr>
        </tbody>
      </table>

      <p>(You can extrapolate the above to apply to emails and names as well)</p>

      <p>Using column oriented storage gives us several advantages:</p>

      <ul>
        <li>
          We can do faster analytical queries over all or a large set of the values in our data for
          just a single column
        </li>
        <li>
          Column compression can also be performed to minimize the amount of data being stored (see
          below)
        </li>
        <li>
          Since we have less data in this scenario, we can even store it in memory or CPU cache for
          even faster reads/writes.
        </li>
      </ul>

      <h3 id="column-compression">Column Compression</h3>

      <p>Imagine we have the following table:</p>

      <table>
        <thead>
          <tr>
            <th>Name</th>
            <th>Followers</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Alice</td>
            <td>3</td>
          </tr>
          <tr>
            <td>Bob</td>
            <td>3</td>
          </tr>
          <tr>
            <td>Charlie</td>
            <td>1</td>
          </tr>
          <tr>
            <td>David</td>
            <td>1</td>
          </tr>
          <tr>
            <td>Edward</td>
            <td>2</td>
          </tr>
          <tr>
            <td>Frank</td>
            <td>5</td>
          </tr>
          <tr>
            <td>Gordon</td>
            <td>4</td>
          </tr>
        </tbody>
      </table>

      <p>
        If we have column oriented storage, we can easily compress the “Followers” file using
        <strong>bitmap encoding</strong>:
      </p>

      <table>
        <thead>
          <tr>
            <th>Followers</th>
            <th>Bitmap Encoding</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>1</td>
            <td>0011000</td>
          </tr>
          <tr>
            <td>2</td>
            <td>0000100</td>
          </tr>
          <tr>
            <td>3</td>
            <td>1100000</td>
          </tr>
          <tr>
            <td>4</td>
            <td>0000001</td>
          </tr>
          <tr>
            <td>5</td>
            <td>0000010</td>
          </tr>
        </tbody>
      </table>

      <p>
        The way this works is: We see that Charlie and David (3rd and 4th in our table) both have
        only 1 Follower, so for the 1 Follower row, we set the 3rd and 4th bits in our bitmap going
        left to right and leave the rest as zeroes. (Hence, 0011000 in the “1 Follower” row)
      </p>

      <p>We can compress this bitmap further by using a <strong>run length encoding</strong>:</p>

      <table>
        <thead>
          <tr>
            <th>Followers</th>
            <th>Bitmap Encoding</th>
            <th>Run-Length Encoding</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>1</td>
            <td>0011000</td>
            <td>223</td>
          </tr>
          <tr>
            <td>2</td>
            <td>0000100</td>
            <td>412</td>
          </tr>
          <tr>
            <td>3</td>
            <td>1100000</td>
            <td>25</td>
          </tr>
          <tr>
            <td>4</td>
            <td>0000001</td>
            <td>61</td>
          </tr>
          <tr>
            <td>5</td>
            <td>0000010</td>
            <td>511</td>
          </tr>
        </tbody>
      </table>

      <p>
        The run length stores the bitmap encoding as a sequence of the count of consecutive zeroes
        and count of consecutive ones. For example, “0011000” is 2 consecutive 0’s, 2 consecutive
        1’s and 3 consecutive 0’s, resulting in “223”.
      </p>

      <p>Performing column compression enables us to:</p>

      <ul>
        <li>Send less data over the network</li>
        <li>
          Potentially keep more data stored in memory or CPU cache if the dataset is small enough
        </li>
      </ul>

      <h3 id="downsides-to-column-oriented-storage">Downsides to column oriented storage</h3>

      <p>There are a few downsides to column oriented storage:</p>

      <ul>
        <li>Every column must have the same sort order.</li>
        <li>
          Writes for a single row need to go to different places on disk (This can be improved via
          an in-memory LSM tree/SSTable set-up since that preserves the sort order of columns).
        </li>
      </ul>

      <h3 id="column-oriented-storage-in-the-wild">Column Oriented Storage in the Wild</h3>

      <ul>
        <li>
          <a href="https://parquet.apache.org/">Apache Parquet</a> is an open-source column-oriented
          data file format designed for efficient storage and retrieval. It provides some nice
          features like:
          <ul>
            <li>
              Metadata containing minimum, maximum, sum, or average values for each data file chunk,
              which enables us to efficiently perform queries
            </li>
            <li>Efficient data compression and encoding schemes</li>
          </ul>
        </li>
        <li>
          <a href="https://aws.amazon.com/redshift/">Amazon Redshift</a> is a column-oriented
          managed data warehouse solution from AWS
        </li>
        <li>
          <a href="https://druid.apache.org/docs//0.21.0/design/index.html">Apache Druid</a> is a
          column-oriented, open source, real-time analytics database
        </li>
      </ul>

      <h2 id="data-serialization-frameworks">Data Serialization Frameworks</h2>

      <p>
        In this section, we’ll take a look at some data serialization formats and their advantages
        and disadvantages
      </p>

      <h3 id="csv">CSV</h3>

      <p>
        CSV stands for “Comma Separated Values”. Data is this format is stored as rows of values
        delimited by commas. CSV is typically easy parse and read, but it doesn’t guarantee type
        safety on the column values or even guarantee values to be present at all:
      </p>

      <p>For example:</p>

      <div class="language-plaintext highlighter-rouge">
        <div class="highlight">
          <pre class="highlight"><code>rownum,name,age,company
1,alice,24,albertsons
2,bob,twenty-five,amazon
3,charlie,22
</code></pre>
        </div>
      </div>

      <p>
        Notice that “bob” has an string-type “age” value, and “charlie” is missing a “company” value
        entirely.
      </p>

      <h3 id="json">JSON</h3>

      <p>
        JSON, or Javascript Object Notation, is a plain text, human-readable format that represents
        data as nested key-value pairs and arrays. JSON is widely used on the web (pretty much every
        language has some kind of JSON-parsing library).
      </p>

      <p>Example of a JSON object</p>

      <div class="language-plaintext highlighter-rouge">
        <div class="highlight">
          <pre class="highlight"><code>{
    "userName": "Mantis Toboggan",
    "age": 80,
    "interests": ["partying", "business"],
    "photoAlbum": [
        {
            "url": "images/01.jpg",
            "width": 200,
            "height": 200
        },
        {
            "url": "images/02.jpg",
            "width": 200,
            "height": 200
        }
    ]
}
</code></pre>
        </div>
      </div>

      <p>
        JSON isn’t the most space-efficient due to repeated keys or duplicated data (the “url”,
        “width”, and “height” tags above, for example), and also doesn’t have type safety
        guarantees.
      </p>

      <h3 id="xml">XML</h3>

      <p>
        XML, or eXtensible Markup Language, is another plain-text, human-readable format that
        structures data as a series of nested tags. It’s similar to HTML, except instead of using a
        set of predefined tags (h1, p, body), it uses custom tags defined by the author.
      </p>

      <p>Example of an XML object:</p>

      <div class="language-plaintext highlighter-rouge">
        <div class="highlight">
          <pre class="highlight"><code>&lt;friendsList&gt;
    &lt;friend&gt;
        &lt;name&gt;Mantis&lt;/name&gt;
        &lt;age&gt;80&lt;/age&gt;
    &lt;/friend&gt;
    &lt;friend&gt;
        &lt;name&gt;Mac&lt;/name&gt;
        &lt;age&gt;47&lt;/age&gt;
    &lt;/friend&gt;
&lt;/friendsList&gt;
</code></pre>
        </div>
      </div>

      <p>
        Like JSON, XML doesn’t guarantee type saftey and isn’t super memory efficient since we have
        repeated tags around all our data.
      </p>

      <h3 id="protocol-buffers-and-thrift">Protocol Buffers and Thrift</h3>

      <p>
        Protocol Buffers and Thrift are binary encoding libraries developed by Google and Facebook
        respectively for serializing data. Here, we define a schema for data and assign each data
        key a numerical tag, which grants smaller data size since we just use these tags instead of
        strings.
      </p>

      <p>Example of a Protocol Buffers schema definition:</p>

      <div class="language-plaintext highlighter-rouge">
        <div class="highlight">
          <pre class="highlight"><code>message Person {
    required string user_name       = 1;
    optional int64  favorite_number = 2;
    repeated string interests       = 3;
}
</code></pre>
        </div>
      </div>

      <p>
        The schemas provide type safety since they require us to specify what type everything is, as
        well as nice documentation for other engineers working with our system. However, writing out
        the schemas require manual dev effort, and the resulting encodings are also not as
        human-readable.
      </p>

      <h3 id="apache-avro">Apache Avro</h3>

      <p>
        <a href="https://avro.apache.org/">Apache Avro</a> is a data serialization format that was
        first developed for Hadoop (which you can read more about
        <a href="/topic/08_batch_processing?subtopic=01_hdfs">here</a>). It uses JSON for defining
        data schemas and serializes to binary. Data in Avro is typed AND named, and columns can be
        compressed to save memory. It can also be read by most languages (with Java being the most
        widely used), though inspecting an Avro document manually requires special Avro tools since
        the data is serialized in binary.
      </p>

      <p>Example of an Avro JSON schema:</p>

      <div class="language-plaintext highlighter-rouge">
        <div class="highlight">
          <pre class="highlight"><code>{
    "type": "record",
    "name": "userInfo",
    "fields": [
        {
            "name": "username",
            "type": "string",
            "default": "NONE"
        },
        {
            "name": "favoriteNumber",
            "type": "int",
            "default": -1
        }
    ]
}
</code></pre>
        </div>
      </div>

      <p>
        One nifty thing Avro can do for us is generate database schemas on the fly based off column
        names and reconcile different reader / writer schemas. For example, if we have two different
        schemas being published by two different servers writing to our database, Avro can
        automatically handle those by filling in missing values between the two schemas with default
        values.
      </p>

      <p>An example scenario:</p>

      <p><strong>Server A’s Schema</strong></p>

      <div class="language-plaintext highlighter-rouge">
        <div class="highlight">
          <pre class="highlight"><code>{
    "type": "record",
    "name": "userInfo",
    "fields": [
        {
            "name": "username",
            "type": "string",
            "default": "NONE"
        },
        {
            "name": "favoriteNumber",
            "type": "int",
            "default": -1
        }
    ]
}
</code></pre>
        </div>
      </div>

      <p><strong>Server B’s Schema</strong></p>

      <div class="language-plaintext highlighter-rouge">
        <div class="highlight">
          <pre class="highlight"><code>{
    "type": "record",
    "name": "userInfo",
    "fields": [
        {
            "name": "username",
            "type": "string",
            "default": "NONE"
        },
        {
            "name": "age",
            "type": "int",
            "default": -1
        }
    ]
}
</code></pre>
        </div>
      </div>

      <p>
        If we try to read data published by Server A using Server B’s schema, Avro will
        automatically <em>ignore</em> the “favoriteNumber” type. Since that data won’t contain an
        “age” column, it will automatically use our default value of -1.
      </p>

      <h2 id="additional-reading--material">Additional Reading / Material</h2>

      <ul>
        <li>
          <em>Designing Data-Intensive Applications</em>, Chapter 3, “Storage and Retrieval”,
          section 3: “Column-Oriented Storage”
        </li>
        <li>
          <em>Designing Data-Intensive Applications</em>, Chapter 4, “Encoding and Evolution”,
          section 1: “Formats for Encoding Data”
        </li>
        <li>
          <strong>jordanhasnolife System Design 2.0 Playlist</strong>:
          <ul>
            <li>
              <a
                href="https://www.youtube.com/watch?v=Zt7rqtJ3uWA&amp;list=PLjTveVh7FakLdTmm42TMxbN8PvVn5g4KJ&amp;index=14"
                >“Column Oriented Storage”</a
              >
            </li>
            <li>
              <a
                href="https://www.youtube.com/watch?v=E7Gk8etqkgU&amp;list=PLjTveVh7FakLdTmm42TMxbN8PvVn5g4KJ&amp;index=15"
                >“Data Serialization Frameworks”</a
              >
            </li>
          </ul>
        </li>
        <li>
          <strong>Stephane Maarek</strong>
          <a href="https://www.youtube.com/watch?v=SZX9DM_gyOE">“Avro Introduction”</a>
        </li>
      </ul>

      <h1 id="database-indexes">Database Indexes</h1>

      <p>
        A database index is a data structure that allows you to efficiently search for specific
        records in your database. Below are a few database index types to know.
      </p>

      <h2 id="hash-index">Hash Index</h2>

      <p>
        In a hash index, we pass each database key into a hash function and store the value at a
        memory address corresponding to the hash. This gives us extremely optimized reads and
        writes, as hash tables provide constant time lookup and storage
      </p>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/01_hash_indexes.png?alt=media&amp;token=f2c44559-f222-4746-b0ae-c5fccffad810"
          alt="Example of a hash index"
        />
      </p>

      <p>
        However, hash indexes are limited to small datasets since the hash of the key might not fit
        within the memory address space. Though you could implement a hash index on disk, it’s not
        efficient to perform random I/O. Furthermore, they’re not particularly good for range
        queries, since the keys aren’t sorted. Grabbing all values across “A” and “B”, for example,
        would require us to either check every possible key (which is infeasible), or iterate
        through all of the keys in our hashmap (which is slow).
      </p>

      <h3 id="hash-indexes-in-the-wild">Hash Indexes in the Wild</h3>

      <ul>
        <li>
          <a href="https://docs.riak.com/riak/kv/2.2.3/setup/planning/backend/bitcask/index.html"
            >Bitcask</a
          >, the default storage engine in <a href="https://riak.com/index.html">Riak</a>, a NoSQL
          key-value data store, uses Hash Indexes
        </li>
      </ul>

      <h2 id="lsm-tree--sstable">LSM Tree + SSTable</h2>

      <p>
        LSM trees are tree data structures used in write-optimized databases. “LSM” stands for “Log
        Structured Merge Tree” and “SSTable” stands for “Sorted String Table”.
      </p>

      <p>
        In general, they’re optimized for write throughput since we’re writing to a data structure
        in memory (fast) before writing to disk (slow). In addition, the sorted nature of SSTables
        allow us to write <em>sequentially</em> to disk which is much faster than writing randomly.
      </p>

      <h3 id="sstable-serialization">SSTable Serialization</h3>

      <p>
        As mentioned before, LSM Trees are auto-balancing binary search trees (e.g. Red-Black trees)
        that we insert database keys into. Once it gets to be a certain size, we flush the LSM tree
        to disk as an SSTable. We do a tree traversal to preserve the sorted ordering of keys
      </p>

      <p>
        Every time we do this SSTable serialization, we create a brand new SSTable which might store
        new values for keys that exist in previous SSTables. We never <em>delete</em> values from
        SSTables, instead we store a marker indicating that the value was deleted called a
        “tombstone”
      </p>

      <ul>
        <li>
          This means that reading might be slow since we’d need to scan every SSTable if the key
          doesn’t exist in our current LSM Tree
        </li>
        <li>
          However, we can merge SSTables together in a background process called SSTable compaction
          (a process similar to the “merge” operation in the mergesort algorithm) reducing the
          number of SSTables we need to search through.
        </li>
      </ul>

      <p>Here’s an example for how this process works:</p>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/02_lsm_tree_sstable.png?alt=media&amp;token=1c10720c-825a-4bf1-ba84-a725ef1f0c0e"
          alt="LSM Tree, Compaction"
        />
      </p>

      <h3 id="write-ahead-logs">Write-Ahead Logs</h3>

      <p>
        A write-ahead log (WAL) is just a log of all the write operations we are doing whenever we
        insert keys into the tree. We maintain a write-ahead log on disk to maintain the durability
        of the LSM tree.
      </p>

      <ul>
        <li>
          <em>Durability</em> means “survivability in the event of failures”. For example, if
          somebody trips on a power cord and wipes our LSM tree in memory, we can recover using the
          operations we recorded in our WAL
        </li>
      </ul>

      <h3 id="lsm-tree--sstables-in-the-wild">LSM Tree + SSTables in the Wild</h3>

      <p>
        The following systems all use LSM Trees (sometimes referred to as a <em>memtable</em>) and
        SSTables
      </p>

      <ul>
        <li>
          <a
            href="https://cassandra.apache.org/doc/latest/cassandra/architecture/storage-engine.html"
            >Apache Cassandra</a
          >, an open source NoSQL database
        </li>
        <li>
          <a href="https://hbase.apache.org/">Apache HBase</a>, the “Hadoop database”, an open
          source, non-relational big data store
        </li>
        <li>
          <a href="https://github.com/google/leveldb/blob/main/doc/impl.md">LevelDB</a>, an open
          source key-value storage library developed by Google
        </li>
        <li>
          <a href="https://rocksdb.org/">RocksDB</a>, a high performance embedded key-value store,
          which is actually a fork of LevelDB developed and maintained by Facebook
        </li>
      </ul>

      <p>
        <strong>Note</strong>: All of the above systems <em>store</em> data as LSM Trees and
        SSTables rather than <em>index</em> data in that way.
      </p>

      <h2 id="b-tree--b-trees">B Tree / B+ Trees</h2>

      <p>
        B Tree indexes are database indexes that utilize a self-balancing N-ary search tree known as
        a B tree, and are the most widely used type of index. Unlike LSM trees and SSTables, B tree
        indexes are stored mostly on disk.
      </p>

      <h3 id="b-tree-properties">B Tree Properties</h3>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/03_btrees.png?alt=media&amp;token=447ef3ab-8dae-4369-8a7b-e4c751da83b5"
          alt="Example B-Tree"
        />
      </p>

      <p>
        B Trees have a special property in that each node contains a range of keys in sorted order,
        and there is a lower and upper bound on the number of keys and children that a node may
        have:
      </p>

      <ul>
        <li>These bounds are usually determined by the size of a page on disk</li>
        <li>
          Inserting into a node might trigger a cascading series of splits to preserve the balance
          of the binary tree, which may incur some CPU penalties. However, the advantages we get in
          terms of read efficiency are typically worth these downsides
        </li>
      </ul>

      <p>
        Reading from B Trees is generally fast due to the high branching factor (large number of
        children at each node). To be more specific, since an internal B tree node can have more
        than just 2 children, the logarithmic time complexity will have a higher base than 2
      </p>

      <h3 id="b-trees">B+ Trees</h3>

      <p>B+ Trees are similar to B Trees, except:</p>

      <ul>
        <li>
          Whereas B Trees can store data at the interior node level, B+ Trees only store data at the
          leaf node level
        </li>
        <li>
          Leaf nodes are linked together (every leaf node has a reference to the next leaf node
          sequentially).
        </li>
      </ul>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/03_bplus_tree.png?alt=media&amp;token=8d33e811-c2c7-4ed4-a191-3c2f18f5c7a2"
          alt="Example B+ Tree"
        />
      </p>

      <p>
        The key advantage offered by this setup is that a full scan of all objects in a tree
        requires just one linear pass through all leaf nodes, as opposed to having to do a full tree
        traversal. The disadvantage is that since only leaf nodes contain data, accessing data might
        take longer since you have to go deeper in the tree
      </p>

      <h3 id="b-trees--b-trees-in-the-wild">B Trees / B+ Trees in the Wild</h3>

      <p>B Trees are used by most relational databases, for example:</p>

      <ul>
        <li>
          <a href="https://dev.mysql.com/doc/refman/8.0/en/innodb-physical-structure.html">InnoDB</a
          >, the storage engine of MySQL, an open source relational DBMS
        </li>
        <li>
          <a
            href="https://learn.microsoft.com/en-us/sql/relational-databases/indexes/indexes?view=sql-server-ver16"
            >Microsoft SQL Server</a
          >, a proprietary DBMS developed and maintained by Microsoft
        </li>
        <li>
          <a href="https://www.postgresql.org/docs/current/btree-implementation.html">PostgreSQL</a
          >, another open source relational DBMS
        </li>
      </ul>

      <p>However, some non-relational databases use them as well, such as</p>

      <ul>
        <li>
          <a href="https://www.mongodb.com/docs/manual/indexes/">MongoDB</a>, a document oriented
          NoSQL database
        </li>
        <li>
          <a href="https://www.oracle.com/database/nosql/technologies/nosql/">Oracle NoSQL</a>, a
          NoSQL, distributed key-value database from Oracle
        </li>
      </ul>

      <h2 id="additional-reading--material-1">Additional Reading / Material</h2>

      <ul>
        <li>
          <em>Designing Data-Intensive Applications</em>, Chapter 3: “Storage and Retrieval”,
          section 1: “Data Structures that Power Your Database”
        </li>
        <li>
          <strong>jordanhasnolife System Design 2.0 Playlist</strong>:
          <ul>
            <li>
              <a
                href="https://www.youtube.com/watch?v=LzNdvuj3a5M&amp;list=PLjTveVh7FakLdTmm42TMxbN8PvVn5g4KJ&amp;index=2"
                >“Database Indexes”</a
              >
            </li>
            <li>
              <a
                href="https://www.youtube.com/watch?v=I1wQsY-Nh_k&amp;list=PLjTveVh7FakLdTmm42TMxbN8PvVn5g4KJ&amp;index=3"
                >“Hash Indexes”</a
              >
            </li>
            <li>
              <a
                href="https://www.youtube.com/watch?v=Z2OaqmxiH20&amp;list=PLjTveVh7FakLdTmm42TMxbN8PvVn5g4KJ&amp;index=4"
                >“B Tree Indexes”</a
              >
            </li>
            <li>
              <a
                href="https://www.youtube.com/watch?v=ciGAVER_erw&amp;list=PLjTveVh7FakLdTmm42TMxbN8PvVn5g4KJ&amp;index=5"
                >“LSM Tree + SSTable Indexes”</a
              >
            </li>
          </ul>
        </li>
        <li>
          <strong>ByteByteGo System Design Video Series</strong>:
          <a href="https://www.youtube.com/watch?v=I6jB0nM9SKU">“The Secret Sauce Behind No SQL”</a>
        </li>
      </ul>

      <h1 id="acid-transactions">ACID Transactions</h1>

      <p>
        ACID is an acronym descripting a set of properties referring to database
        <em>transactions</em>. Transactions are a single unit of work that access and possibly
        modifies the contents of a database. ACID stands for Atomicity, Consistency, Isolation, and
        Durability
      </p>

      <ul>
        <li>
          <strong>Atomicity:</strong> Every transaction either completely succeeds or completely
          fails (no half-measures)
        </li>
        <li>
          <strong>Consistency:</strong> The data in the database is always in a “correct” state. If
          there are database invariants, the data must respect those
        </li>
        <li>
          <strong>Isolation:</strong> Every concurrent database operation must produce the same
          result as if the operations were run in sequential order on a single thread
        </li>
        <li><strong>Durability:</strong> The data can be recovered in the event of failure</li>
      </ul>

      <h2 id="read-committed-isolation">Read-Committed Isolation</h2>

      <p>
        Read committed isolation is the idea that a database query only sees data committed to
        <em>before</em> the query began and never sees uncommitted data or changes committed by
        concurrent transactions. This ensures that our database is protected against dirty writes
        and dirty reads
      </p>

      <ul>
        <li>
          <strong>Dirty writes:</strong> When two concurrent writes conflict with each other and
          cause inconsistency in the database
          <ul>
            <li>
              Solution is row-level locking - if I’m writing to this row, it’s locked and you can’t
              do anything to it until I release that lock
            </li>
          </ul>
        </li>
        <li>
          <strong>Dirty read:</strong> Reading uncommitted data: data is modified by a pending
          write, which causes inconsistency when reading (for example in the event that the write
          fails)
        </li>
      </ul>

      <p>
        In practice, Read Committed isolation can be enforced as an isolation level setting for all
        transactions processed by your DBMS. It provides an <em>intermediate level</em> of isolation
        when compared to other isolation levels:
      </p>

      <ul>
        <li>
          <strong>Read Uncommitted (Low)</strong>: Allows reading uncommitted data (dirty reads)
        </li>
        <li>
          <strong>Read Committed (Intermediate)</strong>: Allows only reading of committed data.
          However, reading a value twice may result in different values (non-repeatable)
        </li>
        <li>
          <strong>Read Repeatable (High)</strong>: Allows only reading of commited data and only by
          a single transaction at a time using exclusive read locks.
        </li>
        <li>
          <strong>Serializable (Highest)</strong>: Serializable execution pretty much guantees that
          transactions appear to be executing in serial order and, by definition, provide the
          highest isolation level
        </li>
      </ul>

      <h2 id="snapshot-isolation">Snapshot Isolation</h2>

      <h3 id="read-skew">Read Skew</h3>

      <p>Consider the following example. We have the following list doctors and patients:</p>

      <div class="language-plaintext highlighter-rouge">
        <div class="highlight">
          <pre class="highlight"><code>Dr. Toboggan: 2
Dr. Phil: 3
Dr. Oz: 2
Dr. Doom: 1
Dr. Patel: 2
</code></pre>
        </div>
      </div>

      <p>
        Let’s assume we have a query that just goes down the list one at a time. We have an
        invariant over the table that there are a total of 10 patients at ALL times.
      </p>

      <p>
        However, let’s now assume that before we begin reading Dr. Oz, one patient transfers from
        Dr. Oz to Dr. Toboggan.
      </p>

      <p>So now, we have the following:</p>

      <div class="language-plaintext highlighter-rouge">
        <div class="highlight">
          <pre class="highlight"><code>Dr. Toboggan: 2 (now updated to 3)
Dr. Phil: 3
Dr. Oz: 1 &lt; READ
Dr. Doom: 1
Dr. Patel: 2
</code></pre>
        </div>
      </div>

      <p>
        But now there’s a problem. We’ve <em>already</em> read that Dr. Toboggan only had 2
        patients. However, due to this write happening in the middle of our read, we now see that a
        patient has gone missing. This state, in which our read is over an inconsistent state of the
        database, is called <strong>read skew</strong>.
      </p>

      <h3 id="snapshots">Snapshots</h3>

      <p>
        Snapshot isolation is a guarantee that all transactions will see a consistent
        <em>snapshot</em>, or state, of the database. This addresses the example that we saw above.
      </p>

      <p>
        Snapshot isolation also guarantees that a write will only successfully commit if it does not
        conflict with any concurrent updates made since that snapshot.
      </p>

      <p>
        Snapshots similar to a write-ahead log, in that they display the last committed values in a
        database for a given point in time. So in our example above, our read query (let’s assume it
        occurs at time T1) would see a snapshot of the database prior to the write, when the patient
        was transferered.
      </p>

      <div class="language-plaintext highlighter-rouge">
        <div class="highlight">
          <pre class="highlight"><code>[T1] Dr. Toboggan: 2
[T1] Dr. Phil: 3
[T1] Dr. Oz: 2
[T1] Dr. Doom: 1
[T1] Dr. Patel: 2
</code></pre>
        </div>
      </div>

      <p>
        Every time we complete a transaction, we store the resulting value for a given key alongside
        a timestamp. We hold on to previous values for a given key so that at any given time we can
        see what the last written value was.
      </p>

      <h2 id="write-skew--phantom-writes">Write Skew &amp; Phantom Writes</h2>

      <p><strong>Write Skew</strong></p>

      <p>
        Write skew occurs when writing a value to the database with an invariant over all the data,
        another concurrent write of which the original write is unaware may put the database in an
        inconsistent state
      </p>

      <p>
        Consider this example where we have a table of doctors with columns “name” and “status”,
        which could be either ACTIVE or INACTIVE:
      </p>

      <table>
        <thead>
          <tr>
            <th>Name</th>
            <th>Status</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Dr. Toboggan</td>
            <td>Active</td>
          </tr>
          <tr>
            <td>Dr. Oz</td>
            <td>Active</td>
          </tr>
          <tr>
            <td>Dr. Phil</td>
            <td>Inactive</td>
          </tr>
        </tbody>
      </table>

      <p>
        We then have an invariant (rule) that at least 1 doctor needs to be active at all times. If
        two transactions concurrently try to set “Dr. Oz” and “Dr. Toboggan” to “INACTIVE”, they’ll
        both read that there are 2 “ACTIVE” doctors allowing each to set its respective doctor to
        “INACTIVE”, violating the invariant.
      </p>

      <p>
        In other words, the end result of BOTH writes violate the invariant. This means row-level
        locking doesn’t work since the invariant is applied over ALL data instead of just one row.
        What we need is a predicate lock over ALL rows affected by the invariant.
      </p>

      <p><strong>Phantom Writes</strong></p>

      <p>
        A phantom write can occur when two concurrent writes try to both add the same new row. No
        locks can be grabbed since the row doesn’t even exist yet, resulting in duplicate rows being
        added to the table.
      </p>

      <p>
        For example, imagine we have a meeting room booking application, where rooms can be booked
        from 11AM to 2PM for 1 hour time slots. Users add a new entry whenever they book a room for
        a time slot:
      </p>

      <table>
        <thead>
          <tr>
            <th>Room</th>
            <th>Time Slot</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Room 1</td>
            <td>1PM - 2PM</td>
          </tr>
          <tr>
            <td>Room 2</td>
            <td>11AM - 12PM</td>
          </tr>
        </tbody>
      </table>

      <p>
        Now if two people try to book Room 2 for 1PM to 2PM at the same time, for example, they
        could potentially <em>both</em> add duplicate rows to the table. That would not be good.
      </p>

      <p>
        One way to mitigate this is to pre-populate the database with all rows that could
        potentially exist. This approach is known as “materializing conflicts”. For example:
      </p>

      <table>
        <thead>
          <tr>
            <th>Room</th>
            <th>Time Slot</th>
            <th>Status</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Room 1</td>
            <td>11AM - 12PM</td>
            <td>RESERVED</td>
          </tr>
          <tr>
            <td>Room 1</td>
            <td>12PM - 1PM</td>
            <td>RESERVED</td>
          </tr>
          <tr>
            <td>Room 1</td>
            <td>1PM - 2PM</td>
            <td>FREE</td>
          </tr>
          <tr>
            <td>Room 2</td>
            <td>11AM - 12PM</td>
            <td>FREE</td>
          </tr>
          <tr>
            <td>Room 2</td>
            <td>12PM - 1PM</td>
            <td>RESERVED</td>
          </tr>
          <tr>
            <td>Room 2</td>
            <td>1PM - 2PM</td>
            <td>RESERVED</td>
          </tr>
        </tbody>
      </table>

      <p>
        Now we can use row level locking to ensure only one user is able to book the room for a
        given time slot!
      </p>

      <p>
        Of course, this approach isn’t feasible for all use-cases. Some storage engines like InnoDB
        provide
        <a href="https://dev.mysql.com/doc/refman/8.0/en/innodb-next-key-locking.html"
          >“next-key locking” to check for duplicate rows</a
        >. However, if your storage engine doesn’t allow this, you may have to enforce serializable
        isolation, preventing concurrent transactions from occurring altogether.
      </p>

      <h2 id="actual-serial-execution">Actual Serial Execution</h2>

      <p>
        Actual Serial Execution forgoes concurrent transactions entirely. Instead, we just process
        transactions in serialized fashion on a single core and try to optimize processing on that
        single core as much as possible. Some of these optimizations could include:
      </p>

      <ul>
        <li>
          Writing everything in memory instead of on disk, enabling us to use things like hash and
          self-balancing tree indexes. This means we can’t store as much data
        </li>
        <li>
          Use stored procedures - save SQL functions in the database and only accept the parameters
          of the function to cut down on the amount of data sent over the network
          <ul>
            <li>
              Stored procedures are somewhat of an antipattern in the real world nowadays since they
              can lead to inflexible and hard-to-maintain code
            </li>
          </ul>
        </li>
      </ul>

      <h4 id="actual-serial-execution-in-the-wild">Actual Serial Execution in the Wild</h4>

      <p>The following systems use Actual Serial Execution to maintain isolation:</p>

      <ul>
        <li>
          <a href="https://www.youtube.com/watch?v=hD5M4a1UVz8">VoltDB</a>, an in-memory database
        </li>
        <li>
          <a href="https://redis.io/docs/interact/transactions/">Redis</a>, an open source in-memory
          store used as a database, cache or message broker
        </li>
        <li>
          <a href="https://www.infoq.com/articles/Architecture-Datomic/">Datomic</a>, a distributed
          database based on the logical query language
          <a href="https://en.wikipedia.org/wiki/Datalog">Datalog</a>
        </li>
      </ul>

      <h2 id="two-phase-locking">Two Phase Locking</h2>

      <p>
        Two Phase Locking is a concurrency control method for enforcing transaction isolation. As
        the name suggests, it’s based on the idea that we have 2 kinds of locks over our database
        rows and we should apply and release them in phases.
      </p>

      <p>The two distinct phases for applying and removing locks are:</p>

      <ol>
        <li>Expanding phase: Locks are acquired and no locks are released.</li>
        <li>Shrinking phase: Locks are released and no locks are acquired.</li>
      </ol>

      <p>The two types of locks we use are:</p>

      <ul>
        <li>
          Read locks (shared lock) for whenever we’re reading data. That prevents writes from
          happening to the row, but still allows other reads
        </li>
        <li>
          Write locks (exclusive lock) for whenever we’re writing data. This prevents both reads and
          other writes
        </li>
      </ul>

      <h3 id="issues-with-two-phase-locking">Issues with Two-Phase Locking</h3>

      <p><strong>Deadlocks</strong></p>

      <p>
        Deadlocks occur when two writes are dependent on each other and neither can release their
        locks until the other does so in turn (a circular dependency). Let’s imagine the following
        scenario:
      </p>

      <p>Two users each maintain a shopping cart:</p>

      <div class="language-plaintext highlighter-rouge">
        <div class="highlight">
          <pre class="highlight"><code>Alice: [Apples, Oranges]
Bob: [Milk, Eggs]
</code></pre>
        </div>
      </div>

      <p>
        Let’s imagine this is some kind of “social shopping” app where each user can see each
        other’s cart. If Alice reads from Bob’s cart and decides she wants to add Bob’s items to her
        own, she will execute a transaction with the following steps.
      </p>

      <ol>
        <li>Grab a read lock on Bob’s cart (to read his items)</li>
        <li>
          Grab a read lock on Alice’s cart (since we need to read in her items before we can update)
        </li>
        <li>Grab a write lock on Alice’s cart to do the update</li>
      </ol>

      <p>However, what if Bob also does the same thing?</p>

      <ol>
        <li>Grab a read lock on Alice’s cart</li>
        <li>Grab a read lock on Bob’s cart</li>
        <li>Gragb a write lock on Bob’s cart to do the update</li>
      </ol>

      <p>
        Now, we have a problem. Neither Alice nor Bob can perform step 3 and grab write locks on
        their own carts to update them, since write locks are exclusive. So in order for Alice to
        grab her write lock, she will need Bob to release his read lock on her cart. But Bob can’t
        do that until <em>his</em> write completes.
      </p>

      <p>
        The only way forward would be for this to be detected by the system, and one transaction
        would be forced to abort.
      </p>

      <p><strong>Phantom writes</strong></p>

      <p>
        As discussed earlier, this is when we try to acquire locks on rows don’t yet exist. Let’s
        imagine we have a table of doctor’s appointments, and each doctor can only see one patient
        at each particular time slot
      </p>

      <table>
        <thead>
          <tr>
            <th>DoctorName</th>
            <th>PatientName</th>
            <th>Time</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Dr. Toboggan</td>
            <td>Charlie</td>
            <td>2:00 PM</td>
          </tr>
          <tr>
            <td>Dr. Toboggan</td>
            <td>Mac</td>
            <td>3:00 PM</td>
          </tr>
          <tr>
            <td>Dr. Oz</td>
            <td>Dennis</td>
            <td>1:00 PM</td>
          </tr>
        </tbody>
      </table>

      <p>
        Let’s imagine 2 patients and both try to schedule a 4:00PM appointment with Dr. Toboggan.
        They would each individually execute the following query:
      </p>

      <div class="language-plaintext highlighter-rouge">
        <div class="highlight">
          <pre
            class="highlight"
          ><code>SELECT * FROM Patients WHERE DoctorName = "Dr. Toboggan" AND Time="4:00PM";
</code></pre>
        </div>
      </div>

      <p>
        2PL would allow both transactions to execute since they’d both be grabbing a shared read
        lock. Now they both grab a write lock for their write queries
      </p>

      <div class="language-plaintext highlighter-rouge">
        <div class="highlight">
          <pre class="highlight"><code>INSERT INTO Patients ("Dr. Toboggan", "Dee", "4:00PM");
INSERT INTO Patients ("Dr. Toboggan", "Cricket", "4:00PM");
</code></pre>
        </div>
      </div>

      <p>
        And since those rows don’t exist yet, 2PL would allow them to do so, violiating consistency.
      </p>

      <p>
        Similar to our write skew example, one way to mitigate this is to use a
        <em>predicate write lock</em> over all rows that meet a certain predicate condition. In this
        case, we want to lock on predicate
        <code class="language-plaintext highlighter-rouge"
          >DoctorName = "Dr. Toboggan" AND Time="4:00PM"</code
        >
      </p>

      <p>
        However, these are slow to run since they require the full query to evaluate. We can do a
        little better if we have an index over the
        <code class="language-plaintext highlighter-rouge">DoctorName</code> column, which allows us
        to grab all of Dr. Toboggan’s patients more efficiently. But then we end up write locking
        Dr. Toboggan’s 2:00PM and 3:00PM appointments
      </p>

      <ul>
        <li>
          Thus, a predicate lock would be <em>pessimisstic</em>, since we lock more rows than we
          actually need.
        </li>
      </ul>

      <h2 id="serializable-snapshot-isolation">Serializable Snapshot Isolation</h2>

      <p>
        The basis of Serializable Snapshot Isolation (SSI) is: instead of pessimistically locking
        over many rows in the database, we can instead just save transaction values to a snapshot
        and then abort and rollback if we detect any inconsistencies. The general flow looks like
        the following:
      </p>

      <ul>
        <li>
          Whenever a write to a row begins, we log the fact that the transaction has begun in our
          snapshot (importantly, this log event indicates that the write has started and NOT that it
          has finished!)
        </li>
        <li>
          When we then try to read that row before the transaction completes, we read the value of
          the row PRIOR to the write (Read committed isolation)
        </li>
        <li>Then, upon the transaction completing, we log a commit event</li>
        <li>
          If we start any operations that depend on an uncommitted value, we’ll have to check to see
          if we need to abort once the value is committed
        </li>
      </ul>

      <p>
        Here’s an example of what this might look like. Assume we have an invariant in our system
        saying we can only add a new appointment for a doctor if their status is ACTIVE
      </p>

      <ul>
        <li><strong>T1</strong>: Read “Dr. Toboggan” is ACTIVE</li>
        <li><strong>T2</strong>: Write “Dr. Toboggan” status to be INACTIVE</li>
        <li>
          <strong>T3</strong>: Read “Dr. Toboggan” is ACTIVE (Note that there’s an uncommitted write
          to this value)
        </li>
        <li><strong>T2</strong>: Commit</li>
        <li>
          <strong>T3</strong>: Add a new appointment row to the appointments table if “Dr. Toboggan”
        </li>
        <li><strong>T3</strong>: Commit (and is aborted by the database)</li>
      </ul>

      <p>
        The abort and rollback process of transactions that conflict is expensive, so SSI is best
        used in situations where most transactions don’t conflict. This allows us to avoid
        unnecessarily locking rows we aren’t writing to. However in other cases where transactions
        are overlapping like this, we should use 2PL.
      </p>

      <h4 id="ssi-in-the-wild">SSI in the Wild</h4>

      <ul>
        <li>
          <a href="https://www.foundationdb.org/files/fdb-paper.pdf">FoundationDB</a>, a free and
          open source distributed NoSQL database designed by Apple, uses SSI
        </li>
      </ul>

      <h3 id="additional-reading--material-2">Additional Reading / Material</h3>

      <ul>
        <li><em>Designing Data-Intensive Applications</em>, Chapter 7: “Transactions”</li>
        <li>
          <strong>jordanhasnolife System Design 2.0 Playlist</strong>:
          <ul>
            <li>
              <a
                href="https://www.youtube.com/watch?v=oGmxzUBCYtY&amp;list=PLjTveVh7FakLdTmm42TMxbN8PvVn5g4KJ&amp;index=7"
                >“Intro to ACID Database Transactions”</a
              >
            </li>
            <li>
              <a
                href="https://www.youtube.com/watch?v=oS60pr8H1e0&amp;list=PLjTveVh7FakLdTmm42TMxbN8PvVn5g4KJ&amp;index=8"
                >“Read Committed Isolation”</a
              >
            </li>
            <li>
              <a
                href="https://www.youtube.com/watch?v=Tgpa9TrxsfU&amp;list=PLjTveVh7FakLdTmm42TMxbN8PvVn5g4KJ&amp;index=9"
                >“Snapshot Isolation”</a
              >
            </li>
            <li>
              <a
                href="https://www.youtube.com/watch?v=eym48yrObhY&amp;list=PLjTveVh7FakLdTmm42TMxbN8PvVn5g4KJ&amp;index=10"
                >“Write Skew and Phantom Writes”</a
              >
            </li>
            <li>
              <a
                href="https://www.youtube.com/watch?v=kN_rOaNZBng&amp;list=PLjTveVh7FakLdTmm42TMxbN8PvVn5g4KJ&amp;index=11"
                >“Achieving ACID: Serial Execution”</a
              >
            </li>
            <li>
              <a
                href="https://www.youtube.com/watch?v=gB7qazeSD3k&amp;list=PLjTveVh7FakLdTmm42TMxbN8PvVn5g4KJ&amp;index=12"
                >“Database Internals: Two Phase Locking”</a
              >
            </li>
            <li>
              <a
                href="https://www.youtube.com/watch?v=4TAKYRzm_dA&amp;list=PLjTveVh7FakLdTmm42TMxbN8PvVn5g4KJ&amp;index=13"
                >“Serializable Snapshot Isolation”</a
              >
            </li>
          </ul>
        </li>
        <li>
          <em>Distributed Computing Musings</em>,
          <a
            href="https://distributed-computing-musings.com/2022/02/transactions-serializable-snapshot-isolation/"
            >“Transactions: Serializable Snapshot Isolation”</a
          >
        </li>
      </ul>

      <h1 id="database-replication">Database Replication</h1>

      <p>
        Database replication is the process of creating copies of a database and storing them across
        various on-premise or cloud destinations.
      </p>

      <p>It provides several key benefits:</p>

      <ul>
        <li>
          Better geographic locality for a global user base, since replicas can be placed closer
          geographically to the users that read from them
        </li>
        <li>Fault tolerance via redundancy</li>
        <li>Better read/write throughput since we split the load across multiple replicas</li>
      </ul>

      <p>There are two types of replication:</p>

      <ul>
        <li>
          <strong>Synchronous replication:</strong> Whenever a new write comes into the system, we
          need to wait for the write to propagate across all nodes before it can be deemed
          successful and allow other transactions.
          <ul>
            <li>Slow but guarantees strong consistency</li>
          </ul>
        </li>
        <li>
          <strong>Asynchronous replication:</strong> Writes might not need to entirely propagate
          through the system before we start other transactions.
          <ul>
            <li>
              Enables faster write throughput but sacrifices consistency (Eventual consistency)
            </li>
          </ul>
        </li>
      </ul>

      <h2 id="improving-eventual-consistency">Improving Eventual Consistency</h2>

      <p>
        Eventual consistency allows us to reap the performance benefit of not having to wait for
        writes to completely propagate through our system before we can do anything else. However,
        there are some issues.
      </p>

      <p>
        One such case is if a user makes a write, but reads the updated value before the write is
        propagated to the replica that they’re reading from, they might read stale data and think
        the application is broken or slow.
      </p>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/stale-read-eventual-consistency.png?alt=media&amp;token=1fba4669-e342-4f21-b065-ab405d60fd0d"
          alt="stale-read"
        />
      </p>

      <p>One way to mitigate this is to <strong>read your own writes</strong>:</p>

      <ul>
        <li>
          Whenever you write to a database replica, read from the same replica for some X time.
        </li>
        <li>
          Set X based on how long it takes to propagate that write to other replicas so that once X
          has passed, we can lift the restriction.
        </li>
      </ul>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/read-your-own-writes.png?alt=media&amp;token=e78195e9-4b41-463e-b2d2-9a5596626371"
          alt="read-your-own-writes"
        />
      </p>

      <p>
        Another issue we could run into is if we read from replicas that are progressively less
        updated, resulting in reading data “out of order”. Let’s look at an example:
      </p>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/out-of-order-reads.png?alt=media&amp;token=1c00a0c1-83c7-4026-9e32-1c5d5d11742b"
          alt="out-of-order"
        />
      </p>

      <p>
        A possible solution for this is to have each user always read data from the same replica.
        This guarantees that our reads are <strong>monotonic reads</strong> - we might still read
        stale data from the same replica, but at least it will be in the correct order.
      </p>

      <h2 id="single-leader-replication">Single Leader Replication</h2>

      <p>
        In a single leader replication (sometimes referred to as Master-Slave or Active-Passive), we
        designate a specific replica node in the database cluster as a leader and write to that node
        only, having it manage the responsibility of propagating writes to other nodes.
      </p>

      <p>
        This guarantees that we won’t have any write conflicts since all writes are processed by
        only one node. However, this also means we have a single point of failure (the leader) and
        slower write throughput since all writes can only go through a single node.
      </p>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/single-leader-replication.png?alt=media&amp;token=0cabadaf-f9eb-4453-b07b-0a09d74457b3"
          alt="single-leader-replication"
        />
      </p>

      <h3 id="possible-failure-scenarios">Possible Failure Scenarios</h3>

      <p>
        In single leader replication, follower failures are pretty easy to recover from since the
        leader can just update the follower after it comes back online. Specifically, the leader can
        see what the follower’s last write was prior to failure in the replication log and backfill
        accordingly.
      </p>

      <p>However, leader failures can result in many issues:</p>

      <ul>
        <li>
          The leader might actually be up, but the follower’s unable to connect due to network
          issues, which would result in it thinking it needs to promote itself to be the new leader
        </li>
        <li>
          A failure might result in lost writes if the leader was in the middle of propagating new
          writes to followers.
        </li>
        <li>
          When a leader comes back online after a new leader has already been determined, we could
          end up with two leaders propagating conflicting writes as clients send writes to both
          nodes (Split brain).
        </li>
      </ul>

      <p>
        In general, single leader replication makes sense in situations where workloads are
        read-heavy rather than write-heavy. We can offload reads to multiple follower nodes and have
        the leader node focus on writes.
      </p>

      <h3 id="single-leader-replication-in-the-wild">Single Leader Replication in the Wild</h3>

      <ul>
        <li>
          Most relational databases like MySQL and PostgreSQL use single-leader replication.
          However, some NoSQL databases like MongoDB, AWS DynamoDB, and RethinkDB support it as well
        </li>
        <li>
          Some distributed message brokers like
          <a
            href="https://blog.rabbitmq.com/posts/2020/07/disaster-recovery-and-high-availability-101/#data-redundancy-tools---back-ups-and-replication"
            >RabbitMQ</a
          >
          and
          <a
            href="https://cwiki.apache.org/confluence/display/kafka/kafka+replication#KafkaReplication-Datareplication"
            >Kafka</a
          >
          (which we’ll talk about when we get to batch and stream processing), also support
          leader-based replication to provide high availability
        </li>
      </ul>

      <h2 id="multi-leader-replication">Multi-Leader Replication</h2>

      <p>
        In multi-leader replication, we write to multiple leader nodes instead of just one. This
        provides some redundancy compared to single-leader replication, as well as higher write
        throughput. It especially makes sense, performance wise, in situations where data needs to
        be available in multiple regions (each region should have its own leader).
      </p>

      <p>There are a few topologies for organizing our write propagation flow between leaders.</p>

      <h3 id="topologies">Topologies</h3>

      <h4 id="circle-topology">Circle Topology</h4>

      <p>
        As the name suggests, leader nodes are arranged in a circular fashion, with each leader node
        passing writes to the next leader node in the circle
      </p>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/multi-leader-circle.png?alt=media&amp;token=78ac3e44-a87f-44a7-bd8d-6af0bdb8f56c"
          alt="circle-topology"
        />
      </p>

      <p>
        If a single node in the circle fails, the previous node in the circle that was passing
        writes no longer knows what to do. Hence, fault tolerance is non-existent in this topology
      </p>

      <h4 id="star-topology">Star Topology</h4>

      <p>
        In a star topology, we designate a central leader node, which outer nodes pass their writes
        to. The central node then propagates these writes to the other nodes
      </p>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/multi-leader-star.png?alt=media&amp;token=e3e4aa81-e50d-4c03-9d31-8bd923404e86"
          alt="star-topology"
        />
      </p>

      <p>
        If the outer nodes die, we’re fine since the central node can continue to communicate with
        the other remaining outer nodes, so it’s a little bit more fault tolerant than the Circle
        Topology. But if the central leader node dies, then we’re screwed
      </p>

      <h4 id="all-to-all-topology">All-to-all Topology</h4>

      <p>
        An all-to-all topology is a “complete graph” structure where every node propagates writes to
        every other node in the system (every node is the “central” node from the Star Topology)
      </p>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/multi-leader-all-to-all.png?alt=media&amp;token=8fe6e9c8-39dc-4419-9596-707ca70707d2"
          alt="all-to-all-topology"
        />
      </p>

      <p>
        This is even more fault tolerant than the star topology since now if any node dies, the rest
        of the nodes can still communicate with each other. However, there are still some issues
        with this toplogy:
      </p>

      <ul>
        <li>
          Since writes are being propagated from every node to every other node, there could be
          cases where duplicate writes get propagated out
        </li>
        <li>
          Writes might not necessarily be in order, which presents an issue if we have causally
          dependent writes (for example, write B modifies a row created by write A)
        </li>
      </ul>

      <p>
        There are some ways to mitigate these issues. We can fix the duplicates issue by keeping
        track in our Replication Log which nodes have seen a given write
      </p>

      <h4 id="solutions-for-write-conflicts">Solutions for Write Conflicts</h4>

      <p>
        Multi leader replication could result in concurrent writes that are unaware of each other,
        causing inconsistency in the database (write conflicts). There are a few solutions for
        mitigating this
      </p>

      <h4 id="conflict-avoidance">Conflict Avoidance</h4>

      <p>
        As the name implies, conflict avoidance just has us avoid conflicts altogether by having
        writes for a particular key only go to one replica. This limits our write throughput, so
        it’s not ideal
      </p>

      <h4 id="last-write-wins">Last Write Wins</h4>

      <p>
        In a last-write wins conflict resolution strategy, we use the timestamp of the write to
        determine what the value of a key should be. The write with the latest timestamp wins
      </p>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/multi-leader-lww.png?alt=media&amp;token=c8f56197-4ec0-4a56-9d7f-1c4168d15f6b"
          alt="last-write-wins"
        />
      </p>

      <p>
        Determining what timestamp to use can be tricky - for one thing, which timestamp do we
        trust? Sender timestamps are unreliable since clients can spoof their timestamp to be years
        in the future and ensure their write always wins.
      </p>

      <p>
        Receiver timestamps, surprisingly, can also be unreliable. Computers rely on quartz crystals
        which vibrate at a specific frequency to determine the time. Due to factors like weather
        conditions and natural degredation, these frequencies can change. This results in computers
        having slightly different clocks over time, a process known as <strong>clock skew</strong>
      </p>

      <ul>
        <li>
          There are some ways to mitigate clock skew, such as using Network Time Protocol (NTP) to
          get a more accurate timestamp from a time server using a GPS clock.
        </li>
        <li>
          However this solution isn’t perfect since we’re subject to network delays if we’re making
          requests to servers
        </li>
      </ul>

      <h4 id="version-vector">Version Vector</h4>

      <p>
        A version vector is just an array that contains the number of writes a given node has seen
        from every other node. For example,
        <code class="language-plaintext highlighter-rouge">[1, 3, 2]</code> represents “1 write from
        partition 1, 3 writes from partition 2, and 2 writes from partition 3”
      </p>

      <p>
        We then use this vector to either merge data together and resolve the conflict or store
        sibling data and offload conflict resolution to the client. Let’s look at an example:
      </p>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/multi-leader-version-vectors.png?alt=media&amp;token=e19a9e6c-adbb-4330-899d-5431f90d5c5c"
          alt="version-vectors"
        />
      </p>

      <h4 id="crdt-conflict-free-replicated-data-types">
        CRDT (Conflict-Free Replicated Data Types)
      </h4>

      <p>
        CRDTs are special data structures that allow the database to easily merge data types to
        resolve write conflicts. Some examples are a counter or a set.
      </p>

      <h5 id="operational-crdts">Operational CRDTs</h5>

      <p>
        Database nodes send operations to each other to keep their data in sync. These have some
        latency benefits compared to state-based CRDTs since we don’t need to send as much data over
        the network.
      </p>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/operational-crdt.png?alt=media&amp;token=bd01ad15-6b75-4ef5-8686-54b319321687"
          alt="operational-crdt"
        />
      </p>

      <p>
        However, there are a few problems with Operational CRDTs. For one thing, they’re not
        <em>idempotent</em>, so don’t do well when we have duplicate or dropped requests. For
        example in the distributed counter above, if we sent that increment operation multiple times
        due to some kind of failure / retry mechanism, we could have some issues.
      </p>

      <p>Furthermore, what if we have <em>causally dependent</em> operations? For example:</p>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/operational-crdt-causal-dep.png?alt=media&amp;token=1492f730-ae84-4082-91d6-c8890e505e75"
          alt="operational-crdt-causal-dep"
        />
      </p>

      <h5 id="state-based-crdts">State-based CRDTs</h5>

      <p>
        Database nodes send the entire CRDT itself, and the nodes “merge” states together to update
        their states. The “merge” logic must be:
      </p>

      <ul>
        <li>
          Associative:
          <code class="language-plaintext highlighter-rouge">f(a, f(b, c)) = f(f(a, b), c)</code>
        </li>
        <li>
          Commutative: <code class="language-plaintext highlighter-rouge">f(a, b) = f(b, a)</code>
        </li>
        <li>
          Idempotent:
          <code class="language-plaintext highlighter-rouge">f(a, b) = f(f(a, b), b)</code>
        </li>
      </ul>

      <p>Let’s take a look at an example.</p>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/state-crdt.png?alt=media&amp;token=a72a6438-b43f-488c-8102-5c1b94aa5630"
          alt="state-crdt"
        />
      </p>

      <p>
        We can see that Node 1 sending that set multiple times would result in the same merged
        result on Node 2. This idempotency also enables state CRDTs to be propagated via the
        <em>Gossip Protocol</em>, in which nodes in the cluster send their CRDTs to some other
        random nodes (which may have already seen the message).
      </p>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/gossip-protocol.png?alt=media&amp;token=df30061a-e938-4a6b-9bb8-36e4de2d8924"
          alt="gossip-protocol"
        />
      </p>

      <h4 id="multi-leader-replication-in-the-wild">Multi-Leader replication in the wild</h4>

      <ul>
        <li>
          CRDTs are used by systems like Redis (an in-memory cache) and Riak (a
          multi-leader/leaderless distributed key-value store).
        </li>
      </ul>

      <h2 id="leaderless-replication">Leaderless Replication</h2>

      <p>
        Leaderless replication forgoes designating specific nodes as leader nodes entirely. In
        leaderless replication, we can write to any node and read from any node. This means we have
        high availability and fault tolerance, since every node is effectively a leader node. It
        also gives us high read AND write throughput.
      </p>

      <h3 id="quorums">Quorums</h3>

      <p>
        To guarantee that we have the most recent values whenever we read from the system, we need a
        <strong>quorum</strong>, which just means “majority”
      </p>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/leaderless-quorums.png?alt=media&amp;token=8f80d154-317f-44f8-8e8e-433008000f6f"
          alt="leaderless-quorum"
        />
      </p>

      <p>
        Writers write to a majority of nodes so that readers can guarantee that at least one of
        their return values will be the most recent when they read from a majority of nodes. In
        mathematical terms:
      </p>

      <div class="language-plaintext highlighter-rouge">
        <div class="highlight">
          <pre
            class="highlight"
          ><code>W (## of nodes we write to) + R (## of nodes we read from) &gt; N (## of total nodes)
</code></pre>
        </div>
      </div>

      <p>
        A nifty trick we can also do with quorums is <strong>read repair</strong>. Whenever we read
        values from R nodes, we might see that some of the results are out of date. We can then
        write the updated value back to their respective nodes.
      </p>

      <p>There are some issues with quorums:</p>

      <ul>
        <li>
          We can still have cases in which writes arrive in different orders to a majority of nodes,
          causing disagreement amongst them as to which one is actually the most recent.
        </li>
        <li>Writes could also just fail, violating that inequality condition we just defined.</li>
      </ul>

      <h4 id="sloppy-quorums">Sloppy Quorums</h4>

      <p>
        Let’s imagine a client is able to talk to <em>some</em> database nodes during a network
        interruption, but not <em>all</em> the nodes it needs to assemble a quorum. We have two
        options here:
      </p>

      <ol>
        <li>Return errors for all requests for which we can’t reach a quorum of nodes</li>
        <li>
          Accept writes anyways, but write them to nodes that <em>are</em> reachable, but which
          aren’t necessarily the nodes that we normally write to.
        </li>
      </ol>

      <p>
        The 2nd option causes a <em>sloppy quorum</em> where the <em>W</em> and <em>R</em> in our
        inequality aren’t among the designated <em>N</em> “home” nodes. Once the original home nodes
        come back up, we need to propagate the writes that were sent to those temporary writer nodes
        back to those home nodes. This process is called <em>hinted handoff</em>. Let’s take a look
        at an example:
      </p>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/leaderless-sloppy-quorums.png?alt=media&amp;token=1be90923-9636-46cc-9113-3919a034db74"
          alt="leaderless-sloppy-quorums"
        />
      </p>

      <h3 id="anti-entropy">Anti-Entropy</h3>

      <p>
        Another way to prevent stale reads is to propagate writes in the background between nodes.
        For example, if node A has writes 1, 2, 3, 4, 5, and node B only has writes 2, 3, 5, we’d
        need the first node to send writes 1 and 4 over.
      </p>

      <p>
        One way to do this is to just send the entire replication log with all the writes from node
        A. But this would be inefficient since all we need is the diff (just writes 1 and 4).
      </p>

      <p>
        We can quickly obtain this diff using a <strong>Merkle Tree</strong>, which is a tree of
        hashes computed over data rows. Each individual row gets hashed to a value, and those values
        are combined and hashed hierarchically until we get a root hash over all the rows.
      </p>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/leaderless-merkle-trees.png?alt=media&amp;token=16a6f055-8700-48c5-9beb-772e14e643ef"
          alt="merkle-tree"
        />
      </p>

      <p>
        Using a binary tree search, we can efficiently identify what’s changed in a dataset by
        comparing hash values. For example, the root hash will tell us if there is any change across
        our entire data set, and we can examine child hashes recursively to track down which
        specific rows have changed.
      </p>

      <h3 id="leaderless-replication-in-the-wild">Leaderless Replication in the Wild</h3>

      <ul>
        <li>
          Amazon’s
          <a href="https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf"
            >Dynamo paper</a
          >
          popularized the leaderless replication architecture. So systems that were inspired by it
          support leaderless replication:
          <ul>
            <li>Apache Cassandra</li>
            <li>Riak</li>
            <li>
              <a href="https://www.project-voldemort.com/voldemort/">Voldemort</a>, a distributed
              key-value store designed by LinkedIn for high scalability
            </li>
          </ul>
        </li>
        <li>
          Interestingly, AWS DynamoDB does <em>NOT</em> use leaderless replication despite having
          <em>Dynamo</em> in the name
        </li>
      </ul>

      <h2 id="additional-reading--material-3">Additional Reading / Material</h2>

      <ul>
        <li><em>Designing Data-Intensive Applications</em>, Chapter 5, “Replication”</li>
        <li>
          <strong>jordanhasnolife System Design 2.0 Playlist</strong>:
          <ul>
            <li>
              <a
                href="https://www.youtube.com/watch?v=FIPCDRRBGz4&amp;list=PLjTveVh7FakLdTmm42TMxbN8PvVn5g4KJ&amp;index=16"
                >“Intro to Replication”</a
              >
            </li>
            <li>
              <a
                href="https://www.youtube.com/watch?v=Y29yuEoBmjM&amp;list=PLjTveVh7FakLdTmm42TMxbN8PvVn5g4KJ&amp;index=17"
                >“Dealing with Stale Reads”</a
              >
            </li>
            <li>
              <a
                href="https://www.youtube.com/watch?v=8h-a7TsXw28&amp;list=PLjTveVh7FakLdTmm42TMxbN8PvVn5g4KJ&amp;index=18"
                >“Single Leader Replication”</a
              >
            </li>
            <li>
              <a
                href="https://www.youtube.com/watch?v=tffuvQtiTwY&amp;list=PLjTveVh7FakLdTmm42TMxbN8PvVn5g4KJ&amp;index=19"
                >“Multi Leader Replication”</a
              >
            </li>
            <li>
              <a
                href="https://www.youtube.com/watch?v=sa4BJAFT8sU&amp;list=PLjTveVh7FakLdTmm42TMxbN8PvVn5g4KJ&amp;index=20"
                >“Dealing with Write Conflicts”</a
              >
            </li>
            <li>
              <a
                href="https://www.youtube.com/watch?v=FG5Varj1Ows&amp;list=PLjTveVh7FakLdTmm42TMxbN8PvVn5g4KJ&amp;index=21"
                >“CRDTs”</a
              >
            </li>
            <li>
              <a
                href="https://www.youtube.com/watch?v=Jy4Cm2WEZVg&amp;list=PLjTveVh7FakLdTmm42TMxbN8PvVn5g4KJ&amp;index=22"
                >“Leaderless Replication”</a
              >
            </li>
            <li>
              <a
                href="https://www.youtube.com/watch?v=DAONthD50g0&amp;list=PLjTveVh7FakLdTmm42TMxbN8PvVn5g4KJ&amp;index=23"
                >“Quorums”</a
              >
            </li>
          </ul>
        </li>
        <li>
          <strong>EnjoyAlgorithms System Design Blog</strong>:
          <ul>
            <li>
              <a
                href="https://www.enjoyalgorithms.com/blog/introduction-to-database-replication-system-design"
                >“Introduction to Database Replication”</a
              >
            </li>
            <li>
              <a href="https://www.enjoyalgorithms.com/blog/master-slave-replication-databases"
                >“Master-Slave Replication (Single Leader Replication)”</a
              >
            </li>
          </ul>
        </li>
      </ul>

      <h1 id="database-partitioning">Database Partitioning</h1>

      <p>
        Once our database starts getting too large, we’ll need to split the data up across different
        pieces (partitions). Complications will arise in terms of how we should best split the
        partition, identifying partitions that a query should be routed to, and re-partitioning in
        the event of failure.
      </p>

      <p>For choosing partitions, we have some different options:</p>

      <ul>
        <li>
          <strong>Range-based partitioning:</strong> Divide our data into ranges (of timestamp or
          name in alphabetical order, for example).
          <ul>
            <li>Enables very fast range queries due to data locality.</li>
            <li>
              Could result in hot spots if a particular range is accessed more often than another.
            </li>
          </ul>
        </li>
        <li>
          <strong>Hash-range based partitioning:</strong> Hash the key and assign it to a partition
          corresponding to the hash.
          <ul>
            <li>Provides a more even distribution of keys, preventing hot spots.</li>
            <li>
              Range queries are no longer as efficient since we don’t have data locality (but we can
              mitigate this using indexes).
            </li>
          </ul>
        </li>
      </ul>

      <h2 id="functional-partitioning-federation">Functional partitioning (Federation)</h2>

      <p>
        Federation (or functional partitioning), splits up databases by function. For example, an
        e-commerce application like Amazon could break up their data into users, reviews, and
        products rather than having one monolothic database that stores everything.
      </p>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/federation.png?alt=media&amp;token=a1906443-075d-4a34-b605-03625b97aa9e"
          alt="federation"
        />
      </p>

      <p>
        Federated databases are accompanied by a
        <strong>Federated Database Management System</strong> (FDBMS), which sits on top of the
        various domain-specific databases and provides a uniform interface for querying and storing
        data. That means users can store and retrieve data from multiple different data sources with
        just a single query. In addition, each of these individual databases is autonomous, managing
        data for its specific function or feature independently of others. Thus, an FDBMS is known
        as a <em>meta</em> database management system.
      </p>

      <p>
        Splitting up data in this manner enables greater flexibility in terms of the types of data
        sources that we use. Different functional domains can use different data storage
        technologies or formats, and our federation layer serves as a layer of abstraction on top.
        That also means we have greater extensibility. When an underlying database in our federated
        system changes, our application can still continue to function normally as long as that
        database is able to integrate with the federated schema.
      </p>

      <p>
        Of course, having this federation layer adds additional complexity to our system, since
        we’ll need to reconcile all these different data stores. Performing joins over different
        databases with potentially different schemas may be complex, for example. Furthermore,
        having to do all this aggregation logic could impact performance.
      </p>

      <h2 id="local--global-secondary-indexes">Local / Global Secondary Indexes</h2>

      <p>
        A <em>secondary index</em> is an additional index that’s stored alongside your primary
        index, which might keep data in a different sort order to improve the performance of certain
        other queries which might not benefit from the sort order of the primary index. There are
        two main types of secondary indexes.
      </p>

      <p>
        <strong>Local secondary indexes</strong> are indexes local to a specific partition on a
        particular column.
      </p>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/local-secondary-index.png?alt=media&amp;token=5e6a97dd-88f3-447e-82a1-a81c373a0be3"
          alt="local-secondary-index"
        />
      </p>

      <ul>
        <li>
          Writing is fairly straightforward - every time we write a value to a partition, we also
          write it into the index on the same node.
        </li>
        <li>
          Reading is slower since we need to scan through the local secondary index of every
          partition and stitch together the result.
        </li>
      </ul>

      <p>
        <strong>Global secondary indexes</strong> are indexes over the entire dataset, split up
        across our partitions.
      </p>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/global-secondary-index.png?alt=media&amp;token=20af19b5-caa7-4109-a1a5-92429e00d6ad"
          alt="global-secondary-index"
        />
      </p>

      <ul>
        <li>
          Reading becomes much faster since we don’t need to query every single partition’s index.
          We can hone in on just the partitions that store whatever range we’re looking for.
        </li>
        <li>
          Writes will become slower since we might end up saving a key on two different partitions
          if the indexed location for that key is not in the same partition as its hash.
        </li>
      </ul>

      <h2 id="consistent-hashing">Consistent Hashing</h2>

      <p>
        You might think a natural way to partition data is with the <em>modulo</em> operator. If we
        have 3 partitions, for example, every key gets assigned to the partition corresponding to
        the hash of that key modulo 3.
      </p>

      <p>
        But then what if one of the partitions goes down? Then we need to repartition across our
        whole dataset since now every key should be assigned to the partition corresponding to its
        hash modulo 2.
      </p>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/mod-partition.png?alt=media&amp;token=0922ba17-c6bc-4d68-bb30-1b4a76a7c33d"
          alt="mod-partition"
        />
      </p>

      <p>
        Is there some way we could rebalance the keys from <em>only</em> the partition that went
        down?
      </p>

      <p>
        We can accomplish this through <strong>consistent hashing</strong>. In this scheme, we
        define hash ranges for every partition in such a way that whenever a partition goes down, we
        can extend the range for the remaining partitions to include the keys that were part of the
        partition that just went down.
      </p>

      <p>
        In the event that the partition comes back online, we just reallocate those keys back to the
        partition in which they originally belonged.
      </p>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/consistent-hashing.png?alt=media&amp;token=00c0520c-9089-4335-8a5a-9a8e169e1ba8"
          alt="consistent-hashing"
        />
      </p>

      <p>
        ByteByteGo also has a great
        <a href="https://youtu.be/UF9Iqmg94tk?si=ReiA315ePHGhSKOR&amp;t=163"
          >video explanation of how this process works</a
        >
      </p>

      <h2 id="fixed-partition-rebalancing">Fixed Partition Rebalancing</h2>

      <p>
        We can also define a fixed number of partitions we have across our entire system rather than
        tying the number of partitions to the number of available nodes.
      </p>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/fixed-partitions.png?alt=media&amp;token=d3c12cd2-3fda-4b02-beca-607bd2b989ca"
          alt="fixed-partition"
        />
      </p>

      <p>
        We can also utilize our system resources more effectively by distributing partitions to
        nodes according to their hardware capacity and performance, e.g. give more of the orphaned
        partitions to the more powerful machines.
      </p>

      <p>
        A downside that might occur with this approach is that as our database grows, the size of
        each partition will grow in turn, since the total number of partitions is static. So that
        means we’d need to pick a good fixed partition number from the outset: one that isn’t too
        large so as to make recovery from node failures expensive and complex, but also one that
        isn’t too small so as to incur too much overhead.
      </p>

      <h2 id="additional-reading--material-4">Additional Reading / Material</h2>

      <ul>
        <li><em>Designing Data-Intensive Applications</em>, Chapter 6, “Partitioning”</li>
        <li>
          <strong>jordanhasnolife System Design 2.0 Playlist</strong>:
          <ul>
            <li>
              <a
                href="https://www.youtube.com/watch?v=Bt8ZMC_Yuys&amp;list=PLjTveVh7FakLdTmm42TMxbN8PvVn5g4KJ&amp;index=25"
                >“Introduction to Partitioning”</a
              >
            </li>
            <li>
              <a
                href="https://www.youtube.com/watch?v=z-xxLoJAfmY&amp;list=PLjTveVh7FakLdTmm42TMxbN8PvVn5g4KJ&amp;index=27"
                >“Consistent Hashing”</a
              >
            </li>
          </ul>
        </li>
        <li>
          <strong>Karan Pratap Singh’s Open-source System Design Course</strong>:
          <a
            href="https://github.com/karanpratapsingh/system-design?tab=readme-ov-file#database-federation"
            >“Database Federation”</a
          >
        </li>
        <li>
          <strong>Saurav Prateek: Systems that Scale</strong>:
          <a href="https://www.linkedin.com/pulse/rebalancing-partitions-strategies-saurav-prateek/"
            >“Rebalancing Partitions Strategies”</a
          >
        </li>
      </ul>

      <h1 id="consistency-and-consensus">Consistency and Consensus</h1>

      <p>
        As we saw with the <em>eventual consistency</em> of asynchronous replication schemes, there
        are no guarantees as to when consistency will actually be achieved. There are ways to
        mitigate this, but sometimes we value strong consistency guarantees over performance and
        fault tolerance.
      </p>

      <p>
        Furthermore, an important abstraction that many distributed systems rely on is the ability
        to agree on something, or to come to a <em>consensus</em>. The key is to be able to do this
        effectively in the face of unreliable networks and failures.
      </p>

      <h2 id="two-phase-commit">Two-Phase Commit</h2>

      <p>
        Two Phase Commit is a way of performing distributed writes or writes across multiple
        partitions while guaranteeing the atomicity of transactions (writes either succeed on every
        partition or fail on every partition).
      </p>

      <p>
        Writing to different partitions is unlike propagating writes across replicas because a
        single transaction to multiple partitions is effectively one logical write split up across
        multiple nodes. Hence this write needs to be atomic.
      </p>

      <p>
        Two-phase commit designates a <em>coordinator node</em> to orchestrate the process. This
        node is usually just the application server performing the cross-partition write.
      </p>

      <p>The following diagram describes the “happy case” flow for two-phase commit.</p>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/2pc.png?alt=media&amp;token=22a4e26f-4128-474b-8c2b-facdb19425bd"
          alt="two-phase-commit"
        />
      </p>

      <p>A few issues can arise:</p>

      <ul>
        <li>
          If a node is unable to commit due to a conflict, the coordinator node will need to send an
          ABORT command to that node and stop the entire process.
        </li>
        <li>
          If the coordinator node goes down, all the partition nodes will hold onto their locks,
          preventing other writes until the coordinator comes back online.
        </li>
        <li>
          If a receiver node goes down after the commit point, the coordinator node will have to
          send a request to commit repeatedly until the receiver comes back online.
        </li>
      </ul>

      <p>
        We generally want to avoid 2PC whenever possible because it’s slow and hard. So try to avoid
        writing across multiple partitions if possible.
      </p>

      <h2 id="linearizable-storage">Linearizable Storage</h2>

      <p>
        Linearizable storage dictates that whenever we perform a read, we can never go back in time
        (we always read the most recent thing). In other words, we want to preserve the ordering of
        writes. This is important for determining who is the most recent person to obtain a lock in
        a distributed locking system or who is the current leader in a single leader database
        replication setup.
      </p>

      <p>
        Coordination services like Zookeeper will need this to keep track of database replication
        leaders and the latest status of application servers, for example.
      </p>

      <p>Let’s take a look at how linearizable storage applies to different replication setups:</p>

      <h3 id="single-leader-replication-setup">Single-leader replication setup</h3>

      <p>
        In single leader replication, log writes are guaranteed to be in order since we only have
        one writer node keeping track of everything. That seems like that would produce linearizable
        storage, right?
      </p>

      <p>
        Turns out, not necessarily! If the client reads from the leader before it can replicate a
        new write over to a follower, and then goes down, that will force the client to read from
        the follower with an outdated value. So this isn’t actually linearizable storage.
      </p>

      <h3 id="multi-leader-or-leaderless-replication-setup">
        Multi-leader or leaderless replication setup
      </h3>

      <p>
        In multi-leader or leaderless replication, writes can go to many places and we could have
        concurrent writes, so we can’t make any guarantees about maintaining a chronological
        ordering (or it doesn’t even make sense to). We should try to achieve a
        <em>consistent</em> ordering across all nodes instead
      </p>

      <p>There are a couple of ways we might do this:</p>

      <p><strong>Version vectors</strong></p>

      <p>
        If we see a higher number of writes across all partitions for a given node, we can determine
        that version vector to be more recent. For example
        <code class="language-plaintext highlighter-rouge">[1, 2, 1]</code> is more recent than
        <code class="language-plaintext highlighter-rouge">[0, 1, 0]</code>
      </p>

      <p>
        We can use some arbitrary mechanism for tie-breakers (interleaved version vector numbers).
        For example, we just pick the one with the higher left-most partition writes as being more
        recent.
      </p>

      <p>
        Version vectors take O(N) space where N is the number of replicas, so it might not be the
        most efficient solution if we have a lot of replicas.
      </p>

      <p><strong>Lamport clocks</strong></p>

      <p>
        A Lamport clock is a counter stored across every replica node which gets incremented on
        every write, both on the client AND the database.
      </p>

      <p>
        Whenever we read the counter value on the client side, we write back the max of the database
        counter and the client counter alongside the write. This value overrides the one on both the
        client and server, guaranteeing that all writes following this one will have a higher
        counter value. When sorting writes, we sort by the counter. If they’re the same, we can sort
        by node number. Let’s see an example of how this might work:
      </p>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/lamport-clocks.png?alt=media&amp;token=bc01dcd0-4f3a-4e66-a41d-b6d3cdb7b43d"
          alt="lamport-clock-example"
        />
      </p>

      <p>
        Unfortunately, we could still end up READING old data if replication hasn’t propagated
        through the system, so this isn’t actually linearizable storage. We only get ordering of
        writes after the fact.
      </p>

      <h3 id="cap-theorem-and-pacelc-theorem">CAP Theorem and PACELC Theorem</h3>

      <p>
        CAP Theorem states that it’s impossible for a distributed system to provide all three of the
        following:
      </p>

      <ul>
        <li>
          <strong>Consistency</strong>: The same response is given to all identical requests. This
          is synonymous with <em>linearizability</em>, in which we provide a consistent ordering of
          writes such that all clients read data in the same order regardless of which replica
          they’re reading from. (Cruciallly it’s NOT the same thing as Consistency in
          <a href="/topic/03_ACID-transactions">ACID</a>)
        </li>
        <li>
          <strong>Availability</strong>: The system is still accessible during partial failures
        </li>
        <li>
          <strong>Partition Tolerance</strong>: Operations remain intact when nodes are unavailable.
        </li>
      </ul>

      <p>
        Historically, CAP Theorem has been used to analyze database technologies and the tradeoffs
        associated with them. For example:
      </p>

      <ul>
        <li>
          “MongoDB’s a CP database! its single leader replication setup enables it to provide
          consistent write ordering, but could experience downtime due to leader failures.”.
        </li>
        <li>
          “Cassandra is an AP database! It has a leaderless or multi-leader replication setup that
          provides greater fault tolerance, but is eventually consistent in the face of write
          conflicts.”
        </li>
      </ul>

      <p>
        However, some have criticized CAP Theorem of being too simple (most notably, Martin
        Kleppmann, author of <em>Designing Data-Intensive Applications</em>).
      </p>

      <p>
        Thus, the <em>PACELC theorem</em> attempts to address some of its shortcomings, preferring
        to frame database systems as either leaning towards latency sensitivity or strong
        consistency. it states:
      </p>

      <blockquote>
        <p>
          “In the case of a network partition (P), one has to choose between either Availability (A)
          and Consistency (C), but else (E), even when the system is running normally in the absence
          of partitions, one has to choose between latency (L) and loss of consistency (C)”
        </p>
      </blockquote>

      <h2 id="distributed-consensus">Distributed Consensus</h2>

      <p>
        The <strong>Raft distributed consensus protocol</strong> provides a way to build a
        distributed log that’s linearizable.
      </p>

      <h3 id="proposing-a-new-leader-in-raft">Proposing a new leader in RAFT</h3>

      <p>Below is the process for electing a new leader in Raft:</p>

      <p>
        One node is designated as a leader and sends heartbeats to all of its followers. If a
        heartbeat is not received after some randomized number of seconds (to prevent simultaneous
        complaints), a follower node will initiate an election.
      </p>

      <p>
        Follower node proposes itself as a candidate for the election corresponding to the next
        term.
      </p>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/RAFT-leader-proposal.png?alt=media&amp;token=e4a51c5c-0845-48f4-8a07-2933bd12ca7f"
          alt="raft-leader-proposal"
        />
      </p>

      <h3 id="leader-election">Leader election</h3>

      <p>The election is held as follows:</p>

      <ul>
        <li>
          A follower from a previous term number (which is determined by the latest write that it’s
          seen) will change itself to a follower of the proposed term number and vote “yes”.
        </li>
        <li>
          A follower who has a write from a newer term number will vote “no” and also update its own
          term number value to the one being proposed if it is higher.
        </li>
        <li>
          If a candidate gets “yes” votes from a quorum of nodes, it wins the election and becomes
          the new leader.
        </li>
      </ul>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/RAFT-leader-election.png?alt=media&amp;token=8d5b5d44-270d-4327-a4e1-25c38ca05ea6"
          alt="raft-leader-election"
        />
      </p>

      <h3 id="raft-write-backfills">Raft Write Backfills</h3>

      <p>
        In Raft, we designate a leader that handles all writes. These writes are recorded in the
        form of logs, which contain an operation and a term number. Given the fact that only one
        node handles writes, it’s possible that followers could have out of date logs.
      </p>

      <p>Raft has 2 invariants when it comes to writes to address out of date logs:</p>

      <ol>
        <li>There is only one leader per term</li>
        <li>Successful writes must make the log fully up to date.</li>
      </ol>

      <p>
        So writes backfill logs in addition to writing new values in order to keep them up to date.
        Furthermore, this invariant guarantees that if two logs are the same at a given point in
        time, every entry prior to that point will be the same
      </p>

      <p>How does this actually work in practice? Let’s look at a diagram of the process:</p>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/RAFT-write-backfills.png?alt=media&amp;token=0bab4089-a7b7-47a1-9d82-4b2b7cd4bf3d"
          alt="raft-write-backfills"
        />
      </p>

      <p>
        In conclusion, Raft is fault-tolerant and creates linearizable storage; however, it is slow
        and shouldn’t be used except in very specific situations where you need write correctness.
      </p>

      <h2 id="additional-reading--material-5">Additional Reading / Material</h2>

      <ul>
        <li>
          <em>Designing Data-Intensive Applications</em>, Chapter 9, “Consistency and Consensus”
        </li>
        <li>
          <strong>jordanhasnolife System Design 2.0 Playlist</strong>:
          <ul>
            <li>
              <a
                href="https://www.youtube.com/watch?v=7DoT2sTGulc&amp;list=PLjTveVh7FakLdTmm42TMxbN8PvVn5g4KJ&amp;index=26"
                >“Two Phase Commit”</a
              >
            </li>
            <li>
              <a
                href="https://www.youtube.com/watch?v=C_XLEeWUq3M&amp;list=PLjTveVh7FakLdTmm42TMxbN8PvVn5g4KJ&amp;index=28"
                >“Linearizable Databases”</a
              >
            </li>
            <li>
              <a
                href="https://www.youtube.com/watch?v=Al2JNJBGG30&amp;list=PLjTveVh7FakLdTmm42TMxbN8PvVn5g4KJ&amp;index=29"
                >“Distributed Consensus - Raft Leader Election”</a
              >
            </li>
            <li>
              <a
                href="https://www.youtube.com/watch?v=FByzF2D_-KU&amp;list=PLjTveVh7FakLdTmm42TMxbN8PvVn5g4KJ&amp;index=30"
                >“Distributed Consensus - Raft Writes”</a
              >
            </li>
          </ul>
        </li>
        <li>
          Martin Kleppmann,
          <a
            href="https://martin.kleppmann.com/2015/05/11/please-stop-calling-databases-cp-or-ap.html"
            >“Please stop calling databases CP or AP”</a
          >
        </li>
        <li>
          Scylla DB Technical Glossary:
          <a href="https://www.scylladb.com/glossary/pacelc-theorem/">“PACELC Theorem”</a>
        </li>
      </ul>

      <h1 id="caching">Caching</h1>

      <p>
        A cache is a hardware or software component that stores data so that future requests for
        that data can be served faster. In the context of an operating system, for example, we have
        CPU hardware caches set up hierarchically (L1, L2, L3) to speed up data access from main
        memory.
      </p>

      <p>In distributed systems, caching provides several important benefits:</p>

      <ol>
        <li>Reduce load on key components</li>
        <li>
          Speed up reads and writes- caches typically store data on memory and are sometimes located
          physically closer to the client.
        </li>
      </ol>

      <p>
        There are some challenges with caching - cache misses are expensive since we have to do a
        network request to the cache, search for the key and discover it’s missing, and then do a
        network request to the database. Furthermore, data consistency on caches is complex. If two
        clients each hit two different caches in two different regions, ensuring the data between
        the two is consistent can be a tough problem to solve.
      </p>

      <h3 id="what-data-do-we-cache">What data do we cache?</h3>

      <p>
        We generally want to cache the results of computationally expensive operations. These could
        include database query results or computations performed on data by application servers.
      </p>

      <p>
        We can also cache popular static content such as HTML, images, or video files. (We’ll talk
        more about this when we get to CDNs)
      </p>

      <h3 id="where-does-our-cache-live">Where does our cache live?</h3>

      <ol>
        <li>
          <p><strong>Local on the application servers themselves</strong></p>

          <ul>
            <li>
              This requires us to use consistent hashing in our load balancer to ensure requests go
              to the same server every time.
            </li>
            <li><em>Pros:</em> Fewer network calls, very fast.</li>
            <li><em>Cons:</em> Cache size is proportional to the number of servers.</li>
          </ul>
        </li>
        <li>
          <p><strong>Global caching layer</strong></p>
          <ul>
            <li><em>Pros:</em> Nodes in the global caching layer can be scaled independently.</li>
            <li><em>Cons:</em> Extra network call, more can go wrong.</li>
          </ul>
        </li>
      </ol>

      <h3 id="caches-in-the-wild">Caches in the Wild</h3>

      <p>There are two popular choices for implementing application caches in software systems:</p>

      <p><strong>Memcached</strong></p>

      <p>
        <a href="https://memcached.org/">Memcached</a> is an open source distributed in-memory
        store. It uses an LRU eviction policy as well as consistent hashing for partitioning data,
        meaning the requests to the same key will be sent to the same partition.
      </p>

      <p>
        Memcached is fairly bare-bones and is more useful for a customized caching implementation
        involving multi-threading or leaderless/multi-leader replication.
      </p>

      <p><strong>Redis</strong></p>

      <p>
        <a href="https://redis.io/">Redis</a> is a more feature-rich in memory store, with support
        for specialized data structures like hash maps, sorted sets, and geo-indexes. It uses a
        fixed number of partitions, re-partitioning using the gossip protocol. It also supports ACID
        transactions using its write ahead log and single threaded serial execution. Its replication
        strategy uses a single leader.
      </p>

      <h2 id="distributed-cache-writes">Distributed Cache Writes</h2>

      <p>There are a few different methods for writing to a distributed cache.</p>

      <h3 id="write-around-cache">Write-Around Cache</h3>

      <p>
        A <strong>write-around</strong> caching strategy entails sending writes directly to the
        database, going “around” the cache.
      </p>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/write-around-cache.png?alt=media&amp;token=242e7e93-c579-48ef-9836-83d1558e2d98"
          alt="write-around-cache"
        />
      </p>

      <p>
        Of course, this means that the corresponding cached value will now be stale. We have a
        couple of ways to fix this:
      </p>

      <ol>
        <li>
          <p>
            Stale Read / TTL - We set a Time To Live (TTL) on the value in the cache, which is like
            an expiration date. Reads to the cache key will be stale until the TTL expires, at which
            point the value will be removed from the cache. A subsequent read on that key will go to
            the database, which will then repopulate the cache and set a new TTL
          </p>
        </li>
        <li>
          <p>
            Invalidation - After writing to the database, we also invalidate its corresponding key
            in the cache. Subsequent reads will trigger the database to repopulate the cached value.
          </p>
        </li>
      </ol>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/write-around-cache-stale-reads.png?alt=media&amp;token=4bdf62f1-5e18-4c23-8fa0-6e7237bab41d"
          alt="stale-reads"
        />
      </p>

      <h3 id="pros-and-cons-of-write-around">Pros and Cons of Write-Around</h3>

      <ul>
        <li>
          <em>Pros:</em> Database is the source of truth, and writes are simple since they’re not
          that different from our usual, non-cache flow
        </li>
        <li>
          <em>Cons:</em> Cache misses are expensive since we need to make 2 network calls, 1 for
          fetching the value from the cache and 1 for fetching the value from the database.
        </li>
      </ul>

      <h3 id="write-through-cache">Write-Through Cache</h3>

      <p>
        A <strong>write-through</strong> caching strategy is when we send writes to the cache, then
        proxy that request to the database. This means we may have inconsistent data between our
        cache and our database. Sometimes that isn’t an issue, but in the cases where it is, we’d
        need to use two phase commit (2PC) to guarantee correctness.
      </p>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/write-through-cache.png?alt=media&amp;token=4629de59-fa5d-4b55-8a72-02301e129823"
          alt="write-through"
        />
      </p>

      <h3 id="pros-and-cons-of-write-through">Pros and Cons of Write-Through</h3>

      <ul>
        <li><em>Pros:</em> Data is consistent between the cache and the database.</li>
        <li><em>Cons:</em> Correctness issues if we don’t use 2PC. If we do, 2PC is slow.</li>
      </ul>

      <h3 id="write-back-cache">Write-Back Cache</h3>

      <p>
        A <strong>write-back</strong> caching strategy is similar to the write-through caching
        strategy, except writes aren’t propagated to the database immediately. We do this mainly to
        optimize for lower write latency since we’re writing to the cache without having to also
        write to the database. The database write-back updates are performed asynchronously - at
        some point down the line, or maybe on a fixed interval, we group writes in the cache and
        send them to the database together.
      </p>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/write-back-cache.png?alt=media&amp;token=bffd175e-b8a8-464f-a372-68b739c1c299"
          alt="write-back"
        />
      </p>

      <p>
        There are a few problems that could occur - for one thing, if the cache just fails, then
        writes never go to the database and we have incorrect data. We can provide better fault
        tolerance through replication, though this may add a lot of complexity to the system.
      </p>

      <p>
        Furthermore, if a user tries to read directly from the database before the write backs can
        happen, they may see a stale value. We can mitigate this with a distributed lock:
      </p>

      <ol>
        <li>
          Whenever we write to the key in our cache, we also grab the distributed lock on that key.
        </li>
        <li>
          We hold the lock until somebody tries to also grab that lock while reading that key in the
          database.
        </li>
        <li>
          This other attempt to grab the lock would trigger a write back from the cache to the
          database, allowing the reader to see the up to date value.
        </li>
      </ol>

      <p>Here’s a diagram of that process:</p>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/write-back-distributed-lock.png?alt=media&amp;token=3246b4b1-0d2c-4cb0-894b-9abce77c20d2"
          alt="distributed-lock-wb-cache"
        />
      </p>

      <p>
        Again, this adds a lot of complexity and latency, so we typically try to avoid needing to do
        this.
      </p>

      <h3 id="pros-and-cons-of-write-back">Pros and Cons of Write-Back</h3>

      <ul>
        <li><em>Pros:</em> Low latency writes.</li>
        <li><em>Cons:</em> We may have stale or incorrect data</li>
      </ul>

      <h2 id="cache-eviction-policies">Cache Eviction Policies</h2>

      <p>
        Our caches have limited storage space, so we’ll need to figure out how best to manage what’s
        stored in the cache and how to get rid of things. Our aim is to minimize cache misses, so
        let’s take a look at how we can best do that.
      </p>

      <h3 id="first-in-first-out">First-in First-out</h3>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/FIFO-eviction.png?alt=media&amp;token=60694623-ecc7-4e8a-bc9c-e76bdf6cf99b"
          alt="fifo-eviction"
        />
      </p>

      <p>
        The first value in the cache becomes the first value to be evicted. It’s relatively simple
        to implement since it’s basically a queue. Every time we add a new value to the cache, we
        delete whatever the oldest value in the cache was.
      </p>

      <p>
        The major problem with this is that we don’t consider data access patterns and might evict
        data that’s still actively being queried. For example, even if many users are querying a
        specific key, that key will eventually become the oldest key added to the cache as new data
        is added. It will subsequently be evicted despite being the most popular key.
      </p>

      <h3 id="least-recently-used">Least Recently Used</h3>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/LRU-eviction.png?alt=media&amp;token=9d2175c6-6c23-4951-a387-7c806fbcfc4a"
          alt="lru-eviction"
        />
      </p>

      <p>
        A better policy for evicting data is the “Least Recently Used” (LRU) policy. This is the
        most commonly used eviction policy in practice. In LRU, the last <em>accessed</em> key gets
        evicted, and more recently used keys get to stay in the cache. This solves the problem we
        saw earlier with First-in First-out - popular keys will remain cached since they will keep
        being used.
      </p>

      <p>
        LRU is a bit more complicated to implement - rather than just having a queue we’d need to
        use a hashmap with a doubly linked list. We need a hashmap to identify the node to move to
        the head of the list, and a doubly-linked list to be able to shift things around in O(1)
        time.
      </p>

      <h3 id="least-frequently-used">Least Frequently Used</h3>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/LFU-eviction.png?alt=media&amp;token=523b0cc4-ae74-4caf-9ce5-a7aee9f6c1de"
          alt="lfu-eviction"
        />
      </p>

      <p>
        A more sophisticated alternative is “Least Frequently Used”, where we keep track of the
        <em>frequencies</em> that a key is being accessed and evict according to that.
      </p>

      <p>
        Using this policy alone also has some problems - if a key is referenced repeatedly for a
        short period of time and then not touched for a long time afterwards, it might stay in the
        cache simply because it has a high frequency count from that initial spike in popularity. In
        addition, new items in the cache might be removed too soon since they start with a low
        frequency counter.
      </p>

      <p>
        Therefore, an LFU policy is typically best for situations in which access patterns of cached
        objects do not change often. For example, the Google logo will always be accessed at a
        fairly high rate, but a hype storm around a new logo for Instagram or Reddit might
        temporarily increase its recency in the cache. Once the hype dies down, those new logos will
        be evicted as their frequency count stops growing, while Google’s remains in the cache.
      </p>

      <h2 id="content-delivery-networks">Content Delivery Networks</h2>

      <p>
        Content Delivery Networks (CDNs), are geographically distributed caches for static content,
        like HTML, image, video, and audio files. These files tend to be large, so we’d like to
        avoid having to download them repeatedly from our application servers or object stores
        (which we’ll get into later).
      </p>

      <p>There are a couple of types of CDNs:</p>

      <h3 id="push-based">Push-based</h3>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/push-cdn.png?alt=media&amp;token=9f45d177-d85f-491f-9e9c-11d4981fa889"
          alt="push-cdn"
        />
      </p>

      <p>
        A <strong>push CDN</strong> pre-emptively populates content that we know will be accessed in
        the CDN. For example, a streaming service like Netflix may have content that comes out every
        month that they’d anticipate their subscribers will consume, so they pre-load it onto their
        CDNs
      </p>

      <h3 id="pull-based">Pull-based</h3>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/pull-cdn.png?alt=media&amp;token=961a2120-ce6f-491a-a8cb-96d831d954a0"
          alt="pull-cdn"
        />
      </p>

      <p>
        A <strong>pull CDN</strong>, in contrast, only populates the CDN with content when a user
        requests it. These are useful for when we don’t know in advance what content will be
        popular.
      </p>

      <h4 id="cdns-in-the-wild">CDNs in the Wild</h4>

      <ul>
        <li>
          <a href="https://www.akamai.com/solutions/content-delivery-network">Akamai</a> provides
          CDN solutions among other edge computing services
        </li>
        <li>
          <a href="https://www.cloudflare.com/application-services/products/cdn/">Cloudflare</a>
          provides a free CDN service
        </li>
        <li>
          <a href="https://aws.amazon.com/cloudfront/">AWS CloudFront</a> is AWS’s CDN offering
        </li>
      </ul>

      <h2 id="additional-reading--material-6">Additional Reading / Material</h2>

      <ul>
        <li>
          <strong>Ilija Eftimov’s Blog</strong>
          <a
            href="https://ieftimov.com/posts/when-why-least-frequently-used-cache-implementation-golang/"
            >“When and Why to use an LFU cache with an implementation in Golang”</a
          >
        </li>
        <li>
          <strong>jordanhasnolife System Design 2.0 Playlist</strong>:
          <ul>
            <li>
              <a
                href="https://www.youtube.com/watch?v=crPoHnhkjFE&amp;list=PLjTveVh7FakLdTmm42TMxbN8PvVn5g4KJ&amp;index=51"
                >“Introduction to Distributed Caching”</a
              >
            </li>
            <li>
              <a
                href="https://www.youtube.com/watch?v=ULgXBImWVWQ&amp;list=PLjTveVh7FakLdTmm42TMxbN8PvVn5g4KJ&amp;index=52"
                >“Distributed Cache Writes”</a
              >
            </li>
            <li>
              <a
                href="https://www.youtube.com/watch?v=4wEQ9_tkqvE&amp;list=PLjTveVh7FakLdTmm42TMxbN8PvVn5g4KJ&amp;index=53"
                >“Cache Eviction Policies”</a
              >
            </li>
            <li>
              <a
                href="https://www.youtube.com/watch?v=4wEQ9_tkqvE&amp;list=PLjTveVh7FakLdTmm42TMxbN8PvVn5g4KJ&amp;index=53"
                >“Redis vs. Memcached: Who Wins?”</a
              >
            </li>
            <li>
              <a
                href="https://www.youtube.com/watch?v=h5YK640kwXY&amp;list=PLjTveVh7FakLdTmm42TMxbN8PvVn5g4KJ&amp;index=55"
                >“Content Delivery Networks”</a
              >
            </li>
          </ul>
        </li>
      </ul>

      <h1 id="batch-processing">Batch Processing</h1>

      <p>
        Batch processing is the method computers use to periodically complete high volume,
        repetitive data jobs. Certain data processing tasks like backups, filtering, and sorting are
        compute intensive and inefficient to run on individual data transactions. Let’s take a look
        at some of the systems we generally use to accomplish this.
      </p>

      <h2 id="hadoop-distributed-file-system-hdfs">Hadoop Distributed File System (HDFS)</h2>

      <p>
        <strong>Hadoop</strong> is a distributed computing framework that is used for data storage
        (Hadoop Distributed File System) and batch processing (MapReduce or Spark).
      </p>

      <p>
        The <strong>Hadoop Distributed File System</strong>, or HDFS, is a distributed, fault
        tolerant file store that’s “rack aware”. That means that we take into account the location
        of every computer we’re storing data on in order to minimize network latency when reading
        and writing files.
      </p>

      <p>
        The architecture of HDFS has two main elements: <strong>Name Nodes</strong>, which are used
        for keeping track of metadata, and <strong>Data Nodes</strong>, which are used for actually
        storing the files.
      </p>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/hdfs-high-level.png?alt=media&amp;token=f74d36c0-2e9d-4e57-9d85-aa424db69d3c"
          alt="hdfs-high-level"
        />
      </p>

      <h3 id="name-node-metadata">Name Node Metadata</h3>

      <p>
        Every name node keeps track of metadata telling us which data node replicas store a given
        file, along with what version of the file the replicas have. This metadata is stored in
        memory (for read performance) with a write ahead log saved on disk for fault tolerance.
      </p>

      <p>
        When a name node starts up, it asks every data node what files it contains and what versions
        they are. Based on this information, it replicates files to data nodes using a configurable
        “replication number”. For example, if we setup HDFS to use a replication number of 3 and the
        name node sees that a file is only stored on 2 data node replicas, it will go ahead and
        replicate that file to a 3rd data node.
      </p>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/name-node-metadata.png?alt=media&amp;token=2f24507a-60e7-4065-8432-e50e62e63a44"
          alt="name-node-metadata"
        />
      </p>

      <h3 id="reading-data-from-hdfs">Reading Data from HDFS</h3>

      <p>
        Generally we expect to read data from HDFS more often than we write it. The reading process
        is as follows:
      </p>

      <ol>
        <li>A client asks a name node for the file location</li>
        <li>The name node replies with the best data node replica for the client to read from</li>
        <li>The client caches the data node replica location</li>
        <li>The client reads the file from that data node replica</li>
      </ol>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/hdfs-reads.png?alt=media&amp;token=8957f7e5-c81b-4fef-b4ea-81aeedb33d4f"
          alt="hdfs-reads"
        />
      </p>

      <p>
        The “rack awareness” feature comes into play in step 2. The name node determines which
        replica is best for the client based on the client’s proximity to it. Once the client
        receives that information, it can just save it in its cache rather than having to ask the
        name node every time.
      </p>

      <h3 id="writing-data-to-hdfs">Writing Data to HDFS</h3>

      <p>
        When we write to HDFS, the name node has to select the replica it writes to in a “rack
        aware” manner. Here’s how that plays out when we’re writing a file for the first time:
      </p>

      <ol>
        <li>A client tells the name node it wants to write a file</li>
        <li>
          The name node will respond with a primary, secondary, and tertiary data node to the client
          based on ascending order of proximity
        </li>
        <li>The client will write their file to the primary data node</li>
      </ol>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/hdfs-writes.png?alt=media&amp;token=7d27f4fc-357f-48ad-9d53-0461b2572a23"
          alt="hdfs-writes"
        />
      </p>

      <p>
        The name node will replicate across data nodes that might be in the same data center in
        order to minimize network latency. For example, the primary and secondary data nodes it
        responds with in step 2 may be in one data center, with the tertiary data node in another.
      </p>

      <h3 id="replication-pipelining">Replication Pipelining</h3>

      <p>
        Notice that the client only writes to one data node in the previous example. HDFS propagates
        the file to the secondary and tertiary data nodes in a process known as “replication
        pipelining”. This process is fairly straightforward: every replica will write to the next
        replica in the chain, e.g. primary writes to secondary, secondary writes to tertiary. On
        each successful write, an acknowledgement will be received. Under normal circumstances, the
        acknowledgement will propagate its way back to the client when the replication has succeeded
        across all data nodes.
      </p>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/hdfs-replication-pipelining.png?alt=media&amp;token=4745ddfa-268c-443d-8d6a-8a50de1751ae"
          alt="replication-pipelining"
        />
      </p>

      <p>
        If there’s a network failure between a primary and a secondary data node for example, the
        client won’t receive this acknowledgement and data might not successfully replicate. The
        client at this point can accept eventual consisteny, or it can continue to retry until it
        receives that acknowledgement. However, it’s not guaranteed that that acknowledgement will
        ever be received. As a result, HDFS cannot be called strongly consistent.
      </p>

      <h3 id="high-availability-hdfs">High Availability HDFS</h3>

      <p>
        Name nodes represent a single point of failure in our system if we only have one of them.
        HDFS solves for this issue by using a coordination service like Zookeeper to keep track of
        backup name nodes. Coordination services are strongly consistent via the use of distributed
        consensus algorithms.
      </p>

      <p>
        If a primary name node fails, its write ahead log operations stored in Zookeeper, will be
        replayed to the secondary name node to reconstruct the metadata information in memory. That
        secondary name node will then be designated as the new primary name node. This replay
        process is known as <em>state machine replication</em>.
      </p>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/high-availability-hdfs.png?alt=media&amp;token=ee07017f-624b-4490-809e-edaa495046ec"
          alt="high-availability-hdfs"
        />
      </p>

      <h3 id="apache-hbase">Apache HBase</h3>

      <p><strong>The Problem with HDFS</strong></p>

      <p>
        One problem with HDFS is that modifying a single value in a file is very inefficient. In
        order to change one key, for example, we’d need to overwrite the entire file, which could be
        on the order of megabytes in terms of size. This is extremely expensive, especially when you
        factor in replication.
      </p>

      <p>
        Apache HBase, an open-source database built on top of HDFS, can help us solve this problem.
        It allows for quick writes and key updates via LSM trees, as well as good batch processing
        functionality due to data locality from column oriented storage and range based
        partitioning, which keeps data with similar keys on the same partition.
      </p>

      <p><strong>Data Model</strong></p>

      <p>
        HBase, similar to Apache Cassandra, is a NoSQL Wide Key Store. Recall that a
        <strong>wide-column database</strong> organizes data storage into flexible columns that can
        be spread across multiple servers or database nodes. Each row has a required cluster and
        sort key, and every other column value after that is optional.
      </p>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/wide-column-store.png?alt=media&amp;token=c60e4a23-3b45-4791-a5de-37a30d6295c5"
          alt="wide-column-store"
        />
      </p>

      <p><strong>Architecture</strong></p>

      <p>
        Similar to HDFS, HBase maintains master nodes and partition nodes. The master node provides
        the same functionality as a name node in HDFS, storing metadata about data nodes and
        directing clients who want to read and write to their optimal data node.
      </p>

      <p>
        The partition node, however, contains an HDFS data node as well as a <em>region node</em>.
        The region node stores and operates the LSM Tree in memory and flushes it to an SSTable
        stored in the data node when it gets to a certain capacity. The data node, as we saw in the
        HDFS section, will then replicate these SSTables to other data nodes in the replication
        pipeline.
      </p>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/apache-hbase.png?alt=media&amp;token=fcbcbf0a-203d-41c6-ac0d-904f38518f26"
          alt="apache-hbase"
        />
      </p>

      <p>
        In addition to this, HBase also uses
        <a href="/topic/01_storage_and_serialization?subtopic=02_column_oriented_storage"
          >column oriented storage</a
        >, which enables it to do large analytical queries and batch processing jobs over all the
        values for a single column efficiently.
      </p>

      <p><strong>When do we use HBase?</strong></p>

      <p>
        HBase is good if you want the flexibility of normal database operations over HDFS, as well
        as optimized batch processing. For most applications that require write optimization,
        Cassandra is probably better. However there are some use cases in which HBase may be a good
        choice - storing application logs for diagnostic and trend analysis, or storing clickstream
        data for downstream analysis, for exxample.
      </p>

      <h2 id="mapreduce">MapReduce</h2>

      <p>
        MapReduce is a programming model or pattern within the Hadoop framework that allows us to
        perform batch processing of big data sets. There are a few advantages to using MapReduce:
      </p>

      <ul>
        <li>We can run arbitrary code with custom mappers and reducers</li>
        <li>
          We can run computations on the same nodes that hold the data, granting us data locality
          benefits
        </li>
        <li>Failed mappers/reducers can be restarted independently</li>
      </ul>

      <p>As the name implies, Mappers and Reducers are the basic building blocks of MapReduce:</p>

      <ul>
        <li><strong>Mappers</strong> take an object and map it to a key-value pairing</li>
        <li>
          <strong>Reducers</strong> take a list of outputs produced by the mapper (over a single
          key) and reduce it to a single key-value pair
        </li>
      </ul>

      <h3 id="architecture">Architecture</h3>

      <p>
        Every data node will have a bunch of unformatted data stored on disk. The MapReduce process
        then proceeds as follows (in memory on each data node):
      </p>

      <ol>
        <li>Map over all the data and turn it into key-value pairs using our Mappers</li>
        <li>
          Sort the keys. We’ll explain why we do this later, but the gist is that it’s easier to
          operate on sorted lists when we reduce.
        </li>
        <li>
          Shuffle the keys by hashing them and sending them to the node corresponding to the hash.
          This will ensure all the key-value pairs with the same key go to the same node. The sorted
          order of the keys is maintained on each node.
        </li>
        <li>
          Reduce the key-value pairs. We’ll have a bunch of key-value pairs at this point that have
          the same key. We want to take all those and reduce it to just a single key-value pair for
          each key.
        </li>
        <li>Materialize the reduced data to disk</li>
      </ol>

      <p>Here’s a diagram of what that process might look like:</p>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/mapreduce-flow.png?alt=media&amp;token=668d47ef-b5e3-4ca1-b63f-8ca58df22223"
          alt="mapreduce-flow"
        />
      </p>

      <p><strong>Why do we sort our keys?</strong></p>

      <p>
        During our reduce operation, if we have a sorted list of keys, we know that once we’ve seen
        the last key in the list, we can just flush the result to disk. Otherwise, we would have to
        store the intermediate result in memory in case we see another tuple for that key.
      </p>

      <p>
        For example, assume we have the following data in our reducer and we’d like to compute the
        sum of values over each key:
      </p>

      <div class="language-plaintext highlighter-rouge">
        <div class="highlight">
          <pre class="highlight"><code>Unsorted:
a: 6
a: 8
b: 3
&lt;- At this point, we need to store the intermediate sum for "a" AND "b" in memory, since we might see more "a's" down the line
b: 7
a: 10
b: 4
</code></pre>
        </div>
      </div>

      <div class="language-plaintext highlighter-rouge">
        <div class="highlight">
          <pre class="highlight"><code>Sorted:
a: 6
a: 8
a: 10
&lt;- Once we've gotten here in our reducer, we can just flush the result for "a" to disk!
b: 3
b: 7
b: 4
</code></pre>
        </div>
      </div>

      <p>Thus, sorting the data is much more memory efficient.</p>

      <p><strong>Job Chaining</strong></p>

      <p>
        Notice that every MapReduce job is just reading in data stored on disk for each data node,
        and then outputting it to data stored on disk at another or the same data node. Given that,
        we can actually just read in the outputs of one MapReduce as an input into another. This job
        chaining process allows us to achieve more complex functionality.
      </p>

      <h2 id="batch-joins">Batch Joins</h2>

      <p>
        During batch computations, we might want to join multiple data sets together. For example,
        if we have some data like the following:
      </p>

      <div class="language-plaintext highlighter-rouge">
        <div class="highlight">
          <pre class="highlight"><code>Favorite Websites:
---------------------
alice: "google.com"
alice: "facebook.com"
bob: "instagram.com"
bob: "reddit.com"

Age / Gender:
---------------------
alice: 22, Female
bob: 23, Male
</code></pre>
        </div>
      </div>

      <p>We might want to produce the following joined result:</p>

      <div class="language-plaintext highlighter-rouge">
        <div class="highlight">
          <pre class="highlight"><code>Favorite Websites with Age/Gender
---------------------------------
alice: "google.com", 22, Female
alice: "facebook.com", 22, Female
bob: "instagram.com", 23, Male
bob: "reddit.com", 23, Male
</code></pre>
        </div>
      </div>

      <p>We have a few ways to do this:</p>

      <p><strong>Sort Merge Join</strong></p>

      <p>
        In a <strong>sort merge join</strong>, we sort the keys in each partition, then hash the
        keys to assign them to the same partition, and then join them together, effectively merging
        sorted lists
      </p>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/sort-merge-join.png?alt=media&amp;token=33413269-e595-4028-9fb3-1b5fb0b90606"
          alt="sort-merge-join"
        />
      </p>

      <p>
        This can be slow since we need to sort all the data by the join key, and we’ll need to send
        at least one whole dataset over the network, possibly both depending on how they are
        partitioned
      </p>

      <p><strong>Broadcast Hash Join</strong></p>

      <p>
        If we have a small dataset, we can send it to all partitions and store them entirely in
        memory as a hash map. That’s the basis for a <strong>Broadcast Hash Join</strong>, in which
        we linearly scan through our large dataset and check it against our hash map to join it.
      </p>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/broadcast-hash-join.png?alt=media&amp;token=c760ce5e-12bc-47dc-a58c-4204c2e44fe3"
          alt="broadcast-hash-join"
        />
      </p>

      <p>
        In this case, we don’t need to sort our large dataset at all, saving a lot of time. Plus,
        sending <em>just</em> the small dataset over the network can be much more efficient.
      </p>

      <p><strong>Partition Hash Join</strong></p>

      <p>
        But what if neither dataset is small enough to fit in memory? That’s where the
        <strong>partition hash join</strong> comes in - we can just <em>partition</em> the datasets
        into smaller datasets so that they <em>do</em> fit in memory. From there, we can just do a
        normal hash join.
      </p>

      <p>
        It’s important here that we partition both our datasets the same way so that the keys in
        each partition correspond to each other and go to the same node.
      </p>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/partition-hash-join.png?alt=media&amp;token=75e0a558-dd40-4172-a0d1-af4bfdf3aa45"
          alt="partition-hash-join"
        />
      </p>

      <h2 id="apache-spark">Apache Spark</h2>

      <p>
        Though MapReduce is a nice framework for doing batch processing in HDFS, it isn’t without
        its problems:
      </p>

      <ul>
        <li>
          Chained jobs in MapReduce are dependent on each other. If one takes a long time, it can
          block another from starting
        </li>
        <li>
          Each job requires a mapper and a reducer. Many times you won’t need more than one mapper,
          so if you have multiple jobs that all only really need 1 mapper you end up with a lot of
          unnecessary sorting
        </li>
        <li>
          Intermediate results of a MapReduce job are stored on disk, so we end up using a lot of
          disk space
        </li>
      </ul>

      <p>Enter Apache Spark, which tries to address these issues:</p>

      <ul>
        <li>
          Nodes in Spark do computations as soon as possible instead of waiting for previous jobs to
          fully complete. As soon as it has all the data it needs, it proceeds to the next step
        </li>
        <li>
          Instead of requiring mappers and reducers, we instead have operator functions (which could
          be mapping or reducing)
        </li>
        <li>
          Spark stores intermediate states in memory instead of on disk. It only stores the input
          and output results on disk
        </li>
      </ul>

      <h3 id="spark-architecture">Spark Architecture</h3>

      <h4 id="resilient-distributed-datasets">Resilient Distributed Datasets</h4>

      <p>
        <em>Resilient Distributed Datasets</em> (RDD)s are a core abstraction in Spark. They are
        immutable, distributed collections of elements of your data that can be operated on in
        parallel.
      </p>

      <p>
        RDDs are abstractions of data collections sourced from multiple partitions and/or data
        stores (e.g. SQL tables, HDFS files, text files). Furthermore, Spark processes RDDs entirely
        in memory, which provides some nice performance benefits.
      </p>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/spark-rdd.png?alt=media&amp;token=59fd8a6f-cfed-46e6-955e-c1abdae5af96"
          alt="spark-rdd"
        />
      </p>

      <h4 id="how-spark-jobs-run">How Spark Jobs Run</h4>

      <p>
        Spark applications are coordinated by a main program (known as a driver), which spins up a
        SparkContext object. The SparkContext object orchestrates tasks by talking to a cluster
        manager service like
        <a href="/topic/13_software_architecture?subtopic=02_containers">Kubernetes</a>, Mesos, or
        Spark’s own standalone cluster manager. It requests “executors” on nodes in the cluster,
        which are processes that perform computations and store data. The SparkContext then forwards
        application code (JAR or Python files) to the executors. Finally, it sends tasks to the
        cluster manager, which will then schedule and run them on the nodes using a Direct Acyclic
        Graph (DAG) Scheduler.
      </p>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/spark-execution.png?alt=media&amp;token=aa10797a-5921-4418-bc01-32408ead9aa7"
          alt="spark-execution"
        />
      </p>

      <h4 id="fault-tolerance">Fault Tolerance</h4>

      <p>
        Of course, the fact that Spark does everything in memory naturally raises concerns about
        fault-tolerance. Fortunately, it has some mechanisms to ensure that we can recover from
        faults:
      </p>

      <p>
        For <strong>narrow dependencies</strong> where computations on a given node don’t depend on
        data from other nodes, if a node goes down, its workload can be split across the remaining
        nodes and re-processed in parallel
      </p>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/spark-narrow-dependencies.png?alt=media&amp;token=a31fd10a-e87d-4c39-8d6b-2f598e47fd11"
          alt="narrow-dependency"
        />
      </p>

      <p>
        For <strong>wide dependencies</strong> where computations on a node depend on data from
        multiple other nodes, the process is much more tedious. If a node fails, its upstream states
        will need to be recomputed. Luckily, Spark automatically checkpoints data to disk after a
        wide dependency.
      </p>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/spark-wide-dependency.png?alt=media&amp;token=c6b42c7b-9285-4a8c-9675-8feceb90b65f"
          alt="wide-dependency"
        />
      </p>

      <h2 id="additional-reading--material-7">Additional Reading / Material</h2>

      <ul>
        <li><em>Designing Data-Intensive Applications</em>, Chapter 10, “Batch Processing”</li>
        <li>
          <strong>jordanhasnolife System Design 2.0 Playlist</strong>:
          <ul>
            <li>
              <a
                href="https://www.youtube.com/watch?v=ix88Zj0asjs&amp;list=PLjTveVh7FakLdTmm42TMxbN8PvVn5g4KJ&amp;index=37"
                >“WTF is Hadoop?”</a
              >
            </li>
            <li>
              <a
                href="https://www.youtube.com/watch?v=ouxCE6ViVpw&amp;list=PLjTveVh7FakLdTmm42TMxbN8PvVn5g4KJ&amp;index=38"
                >“What’s HBase and How does it compare to Cassandra?”</a
              >
            </li>
            <li>
              <a
                href="https://www.youtube.com/watch?v=lHp7M078nHo&amp;list=PLjTveVh7FakLdTmm42TMxbN8PvVn5g4KJ&amp;index=39"
                >“WTF is MapReduce?”</a
              >
            </li>
            <li>
              <a
                href="https://www.youtube.com/watch?v=gqxbQTVgdkI&amp;list=PLjTveVh7FakLdTmm42TMxbN8PvVn5g4KJ&amp;index=40"
                >“The <em>Right</em> Way to do Batch Job Data Joins”</a
              >
            </li>
          </ul>
        </li>
      </ul>

      <h1 id="stream-processing">Stream Processing</h1>

      <p>
        Unlike in batch processing, stream processing deals with <em>unbounded data</em>, or data
        that arrives gradually over time and is continually produced.
      </p>

      <p>
        The general structure of stream processing systems involves two main stakeholders: producers
        and consumers. Producers, as the name implies, produce events. Consumers then consume those
        events.
      </p>

      <h2 id="message-brokers">Message Brokers</h2>

      <p>
        Instead of having many long lived connections between producers and consumers to handle
        event propagation, we have a <em>message broker</em>, which takes on the responsibility of
        ingesting events from producers and pushing them to consumers. The typical underlying
        implementation of these are queues.
      </p>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/message-brokers.png?alt=media&amp;token=01e1e11d-1af5-424f-924e-4e9b02fcda20"
          alt="message-brokers"
        />
      </p>

      <p>
        There are two types of message brokers: in-memory message brokers, and log-based message
        brokers
      </p>

      <h3 id="in-memory-message-brokers">In-memory Message Brokers</h3>

      <p><em>Examples: RabbitMQ, ActiveMQ, Azure Service Bus, Google Cloud Pub/Sub</em></p>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/in-memory-message-broker.png?alt=media&amp;token=3b200a00-e01c-4496-b48f-3de3ae1a4b88"
          alt="in-memory-message-broker"
        />
      </p>

      <p>
        In-memory message brokers keep all messages in memory and typically optimize for higher
        throughput at the expense of ordered event processing and durability. Some typical use cases
        could include encoding user videos that get posted to Youtube, or sending user tweets to the
        news feeds of their followers
      </p>

      <p>
        When a message gets delivered, it is marked for deletion from the queue. Once the consumer
        that received the message sends an ack that it was processed, the message is actually
        deleted. This could result in fault tolerance issues - if a message queue goes down, we
        don’t have any way of replaying messages.
      </p>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/in-memory-broker-deletion.png?alt=media&amp;token=ec3573e0-ca09-449c-8102-cd21125eb457"
          alt="in-memory-deletion"
        />
      </p>

      <p>
        Messages are delivered to consumers in a round robin fashion. This means that if we have
        multiple consumers reading messages, they may not necessarily be processed in order. If the
        first consumer has a slow network connection or takes longer to process the message, the
        second consumer may end up finishing processing the subsequent message in the queue faster.
      </p>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/in-memory-out-of-order.png?alt=media&amp;token=47357746-6ae6-4ebd-9ac1-bc3416c38b61"
          alt="in-memory-out-of-order"
        />
      </p>

      <p>
        One way to avoid this is with <strong>fan out</strong>, where we partition our queue into
        multiple queues and have each consumer exclusively read from one queue. However, this would
        limit our throughput which kind of defeats the purpose of using an in-memory broker.
      </p>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/in-memory-fanout.png?alt=media&amp;token=7e0e51ce-a396-424d-85b9-486d8cafaf68"
          alt="in-memory-fanout"
        />
      </p>

      <h3 id="log-based-message-brokers">Log-based Message Brokers</h3>

      <p><em>Examples: Apache Kafka, Amazon Kinesis Streams</em></p>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/log-based-message-broker.png?alt=media&amp;token=56b04f13-ee10-4948-a39b-e82b15ca15d8"
          alt="log-based-message-broker"
        />
      </p>

      <p>
        A log-based message broker keeps all its messages sequentially on disk and does not delete
        messages after they’ve been consumed. That inherently gives us more durability. Some use
        cases for these are processing a running average of sensor metrics, or doing change data
        capture - keeping writes from a database in sync with another derived data store.
      </p>

      <p>
        Every message in a log-based message broker will be stored according to the order in which
        it arrived. Furthermore, log based brokers keep track of which messages consumers have seen
        to determine what message to send next. That means every message is processed in order, so
        one slow to process message could potentially slow down a consumer’s overall read
        throughput. Of course, we could mitigate this with partitioning, similar to the fan out
        method in which other consumers read from other queues.
      </p>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/log-based-throughput.png?alt=media&amp;token=88efc31f-4d91-4e95-9512-dd370a7c5216"
          alt="log-based-throughput"
        />
      </p>

      <h3 id="exactly-once-message-processing">Exactly once message processing</h3>

      <p>
        Exactly once message processing refers to the idea that we send a message both at least once
        <em>and</em> no more than once
      </p>

      <p>
        <strong>At least once</strong>: In order to guarantee that a message is sent at least once,
        our message broker will need to be fault tolerant so that it can persist messages and resend
        in the event of failure. It will also need consumer acknowledgement that the message was
        received - if there are network issues and a message is not delivered successfully (lack of
        acknowledgement), the broker can retry.
      </p>

      <p>
        <strong>No more than once</strong>: To ensure that a message is not sent more than once, we
        could use Two Phase Commit to perform a distributed transaction, guaranteeing that a
        consumer received and processed the message and then deleting it from the broker afterwards.
      </p>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/2pc-exactly-once.png?alt=media&amp;token=5c9dbe4e-7fc5-4839-b018-0c42d2710fb0"
          alt="2PC-exactly-once"
        />
      </p>

      <p>
        Otherwise, we could send the message multiple times but ensure that it isn’t
        <em>processed</em> more than once. In other words, messages are idempotent - sending
        duplicates of the same message yields the same result as sending the message only once.
      </p>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/idempotence-exactly-once.png?alt=media&amp;token=f261f757-583c-477c-b828-8644755cff26"
          alt="idempotence"
        />
      </p>

      <h2 id="stream-processing-use-cases">Stream Processing Use Cases</h2>

      <p>Here are a few common stream processing use cases:</p>

      <p><strong>Metric/Log Time Grouping and Bucketing</strong></p>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/metric-log-grouping.png?alt=media&amp;token=c8c6f076-1317-4d47-a08c-e51e8a50e8bf"
          alt="metric-log-grouping"
        />
      </p>

      <p>
        In Metric/Log time grouping and bucketing, we want to create time interval buckets (or
        “tumbling windows”) and group events according to their timestamp
      </p>

      <p>
        Sometimes we want to aggregate over many of these intervals in our analytics (hopping
        window). In this case, we can use a hashmap based on our time interval (for example, one
        minute), and group everything that occurs within that interval together.
      </p>

      <p>
        Other times, we just want a sliding window (get me the events that occurred in the last 5
        minutes). In this case, we can use a queue to track events as they arrive and evict old
        events that fall outside of our window
      </p>

      <p><strong>Change Data Capture</strong></p>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/change-data-capture.png?alt=media&amp;token=d1302e09-40af-4e4f-844a-81749e3fb27c"
          alt="change-data-capture"
        />
      </p>

      <p>
        The idea behind Change Data Capture (CDC), is this: whenever we write to our database, we
        also want to store data derived from that change somewhere else (like a search index)
      </p>

      <p>
        In practice, this means that writes to our database will publish an event to the message
        broker, which then updates the derived data store and keeps it in sync. This prevents us
        from needing to do two-phase commit between the database and the derived data store.
      </p>

      <p><strong>Event Sourcing</strong></p>

      <p>
        <img
          src="https://firebasestorage.googleapis.com/v0/b/system-design-daily.appspot.com/o/event-sourcing.png?alt=media&amp;token=2fd76598-08e9-4dcd-9558-eadf2837422a"
          alt="event-sourcing"
        />
      </p>

      <p>
        Event sourcing is similar to Change Data Capture, except that instead of writing to a
        database and then propagating those changes <em>via</em> a message broker to another data
        store, we write directly to the message broker itself.
      </p>

      <p>
        The message broker can then surface events to any variety of consumers, which can then store
        them in a database.
      </p>

      <p>
        An important thing to note is that events that get stored in message brokers are
        <em>database agnostic</em>, whereas in CDC data that’s landing the message broker is
        <em>database specific</em>. This allows us to easily upgrade or switch out the databases
        that eventually store these events as long as we provide the translation logic to do so from
        our broker.
      </p>

      <p>
        Event sourcing inherently assumes that the message broker is holding on to events. So that
        means we’d need a log-based message broker in order to facillitate this.
      </p>

      <h2 id="stream-enrichment">Stream Enrichment</h2>

      <p>
        Stream enrichment is when we want to augment our stream events with more information and
        send them to other parts of our system. In other words, we want to <em>join</em> data from
        our streams with data from other sources (which themselves could be streams or databases) to
        produce some kind of aggregated or combined data.
      </p>

      <h3 id="stream-stream-joins">Stream-Stream Joins</h3>

      <p>
        In a stream-stream join, we want to join data from two streams together. For example, let’s
        say we have two event producers: one for “Google Search Terms” and another for “Links
        clicked”. We want to join events from the two so that we can correlate search terms with the
        resulting links that were clicked by a given user.
      </p>

      <p>
        Since events from both producers might not come at the same time through our message broker,
        the consumer needs to cache events in memory, and match them with other events that
        correspond to the same key. In the example above, a user
        <code class="language-plaintext highlighter-rouge">Alice</code> may search the term
        “tissues”, and the resulting link clicked event may not come until later. So the consumer
        would need to hold on to the
        <code class="language-plaintext highlighter-rouge">{ "Alice": tissues" }</code> search term
        mapping until it sees the corresponding
        <code class="language-plaintext highlighter-rouge">{ "Alice": "amazon.com" }</code> event
        and join the two.
      </p>

      <h3 id="stream-table-joins">Stream-Table Joins</h3>

      <p>
        In a stream table join, we now need to join events from a stream produce with data from a
        database. To use the previous example, let’s say we had a “Demographics” table telling us
        the age and gender of our users. We want to join the “Google Search Terms” producer with our
        “Demographics” table.
      </p>

      <p>
        A naive way to do this is to have our consumer query the database every time it receives a
        “Google Search Terms” event. This is inefficient since we’d need to perform a network
        request to the database every time.
      </p>

      <p>
        We can do this more efficiently by storing an in-memory copy of our “Demographics” table in
        our consumer and using Change Data Capture to keep it in sync with the “Demographics” table
        in our database. In other words, we make a new write to the database, we propagate that
        change to our consumer using a message broker.
      </p>

      <p>
        We can then use that in-memory table to perform the join whenever we receive an event from
        our “Google Search Terms” producer.
      </p>

      <h3 id="table-table-joins">Table-Table Joins</h3>

      <p>
        In a table-table join, we want to get join results as tables change. This is different from
        just a normal join, which is a static, one-shot operation.
      </p>

      <p>
        To do this efficiently, we can use Change Data Capture in a similar fashion to our
        stream-table join example. We keep two in-memory copies of our tables in our consumer, and
        then perform CDC for each table to keep those in-memory tables in sync. Of course, two
        tables might be too big to fit in memory for one consumer. In that case, we’d need to
        partition our tables by the join key and perform CDC across multiple consumers.
      </p>

      <h2 id="stream-processing-frameworks">Stream Processing Frameworks</h2>

      <p>
        As we saw in the previous join examples, we tend to need to keep state in memory in the
        consumer. As a result, we need a way to ensure fault tolerance in our consumers. Stream
        processing frameworks have mechanisms to ensure just that. Some examples of them include
        Flink, Spark Streaming, Tez, and Storm.
      </p>

      <p>
        Note that all of these stream processing frameworks aren’t the message queues, rather,
        they’re the consumers.
      </p>

      <h3 id="apache-flink">Apache Flink</h3>

      <p>
        Apache Flink is an open source, unified stream and batch processing framework. It allows us
        to have fault tolerance in our consumers and guarantees that each message only affect the
        state of each consumer once. An important caveat is that this guarantee only applies to
        consumers within the confines of our stream framework. We cannot guarantee that state
        outside of the framework will be affected only once when we replay messages after a crash.
      </p>

      <p><strong>Why is Fault Tolerance Hard?</strong></p>

      <p>
        Let’s imagine a scenario where a consumer reads a message from queue A, performs some kind
        of stream join, and publishes the result to queue B. Before it’s able to respond to queue A
        that it’s successfully processed the message, it goes down. This means that queue A never
        moves to the next message in the queue, and so when the consumer comes back up, the same
        message will be processed and pushed to queue B again.
      </p>

      <p><strong>Flink Fault Tolerance</strong></p>

      <p>
        Flink solves this issue by saving checkpoints of each consumer on some form of durable
        storage (such as HDFS). These checkpoints contain the state of every consumer, so in the
        event of failures, the consumer states can be rebuilt from those checkpoints. Furthermore,
        since we’re using log-based queues in these kinds of scenarios, Flink can replay the
        messages that occurred from our checkpoint onwards.
      </p>

      <p>
        The way checkpoints are saved are via <em>barrier messages</em>. Flink designates a Job
        Manager node in our system that occasionally sends a barrier message through our queue,
        which, when received by a consumer, triggers a checkpoint to be created and saved.
      </p>

      <p>
        These barrier messages propagate through all the consumers in our system, creating
        checkpoints that are <em>causally consistent</em>. Checkpoints for a consumer
        <em>only</em> get created if it recieves a barrier message from all of its upstream queues
        its reading from. Therefore, we are guaranteed that for a given checkpoint, all of the
        messages that have been processed by every consumer in our system are the same.
      </p>

      <h2 id="additional-reading--material-8">Additional Reading / Material</h2>

      <ul>
        <li><em>Designing Data-Intensive Applications</em>, Chapter 11, “Stream Processing”</li>
        <li>
          <strong>jordanhasnolife System Design 2.0 Playlist</strong>:
          <ul>
            <li>
              <a
                href="https://www.youtube.com/watch?v=7PjPhgCoT9c&amp;list=PLjTveVh7FakLdTmm42TMxbN8PvVn5g4KJ&amp;index=42&amp;pp=iAQB"
                >“What’s Stream Processing + When Do We Use It?”</a
              >
            </li>
            <li>
              <a
                href="https://www.youtube.com/watch?v=_5mu7lZz5X4&amp;list=PLjTveVh7FakLdTmm42TMxbN8PvVn5g4KJ&amp;index=43&amp;pp=iAQB"
                >“Kafka v.s. RabbitMQ”</a
              >
            </li>
            <li>
              <a
                href="https://www.youtube.com/watch?v=oiPCC8G6ufg&amp;list=PLjTveVh7FakLdTmm42TMxbN8PvVn5g4KJ&amp;index=44&amp;t=317s&amp;pp=iAQB"
                >“Stop messing up your stream processing joins!”</a
              >
            </li>
            <li>
              <a
                href="https://www.youtube.com/watch?v=fYO5-6Owt0w&amp;list=PLjTveVh7FakLdTmm42TMxbN8PvVn5g4KJ&amp;index=45&amp;pp=iAQB"
                >“Apache Flink - A must-have for your streams”</a
              >
            </li>
          </ul>
        </li>
      </ul>

      <h1 id="specialized-data-stores-and-indexes">Specialized Data Stores and Indexes</h1>

      <p>
        Sometimes, a traditional database or index isn’t the optimal solution for certain situations
        (e.g. searching, tracking time series data, maintaining social graphs, or working with
        geographical data). This section will look at a variety of data stores and indexes for
        specialized use cases.
      </p>

      <h2 id="search-indexes">Search Indexes</h2>

      <p>
        Database indexes aren’t very good for text searching because our search term might be any
        single substring of text stored in our DB
      </p>

      <p>
        For example, a traditional index isn’t going to help us search “Apple” in a dataset that
        contains “Apple”, “Cherry”, and “Green Apple” efficiently since “Cherry” comes before “Green
        Apple” lexicographically. Our index would return the ordering “Apple”, “Cherry”, and “Green
        Apple” when we want “Apple” and “Green Apple” to be next to each other
      </p>

      <p>
        Search indices solve this problem using <em>inverted indexes</em>. Let’s take a look at a
        few:
      </p>

      <p><strong>Prefix Inverted Index</strong></p>

      <p>
        In a prefix inverted index, we tokenize all the strings in our data and pair them with
        references to documents which contain those string tokens. To use our example from before,
        “Apple” would be a token, and the ids of documents containing “Apple” and “Green Apple”
        would be saved as references alongside that token.
      </p>

      <p><strong>Suffix Inverted Index</strong></p>

      <p>
        A suffix inverted index is essentially the same as prefix-inverted indexes, except we store
        reversed tokens. It allows us to easily search suffixes by using the reversed the suffix as
        our search term string and proceeding as normal
      </p>

      <p>
        <strong>Apache Lucene</strong> is a popular open source search engine that supports complex
        text searches, Levenshtein distance, numerical searches, etc.
      </p>

      <h4 id="elasticsearch">Elasticsearch</h4>

      <p>
        Elasticsearch is a convenience wrapper on top of Apache Lucene to allow for searching in a
        distributed system. It also provides some nice features out of the box:
      </p>

      <ul>
        <li>REST API</li>
        <li>Query language</li>
        <li>Managed replication and partitioning</li>
        <li>Visualization</li>
        <li>
          Uses local indexes per partition instead of a global index to reduce data duplication
          <ul>
            <li>
              Generally wants to keep searches to one partition instead of aggregating search
              results across many partitions
            </li>
          </ul>
        </li>
        <li>
          Smart caching
          <ul>
            <li>
              Cache more common/frequently accessed parts of a query instead of the full query
              result or a piece of an index
            </li>
          </ul>
        </li>
      </ul>

      <h2 id="time-series-databases">Time Series Databases</h2>

      <p>
        A time series database is a database optimized for reading and writing time-series data. It
        uses column-oriented storage, which as we’ve seen previously, is better optimized for
        analytical queries over a single column.
      </p>

      <p>
        Time series databases use hypertables to optimize reads and writes. A
        <strong>hypertable</strong> is a way to paramterize our data based on time and source. For
        example, we might have 3 sensors and 3 chunks of time (1-2PM, 2-3PM, 3-4PM). We can image a
        hypertable as being a 3x3 grid with sensors along the X-axis and time intervals along the
        Y-axis:
      </p>

      <table>
        <thead>
          <tr>
            <th></th>
            <th>Sensor 1</th>
            <th>Sensor2</th>
            <th>Sensor3</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>1-2PM</td>
            <td></td>
            <td></td>
            <td></td>
          </tr>
          <tr>
            <td>2-3PM</td>
            <td></td>
            <td></td>
            <td></td>
          </tr>
          <tr>
            <td>3-4PM</td>
            <td></td>
            <td></td>
            <td></td>
          </tr>
        </tbody>
      </table>

      <p>
        Each cell in the table above would correspond to a chunk of data for a given sensor between
        a given time interval.
      </p>

      <p>
        Breaking data up in this way allows us to cache much more efficiently, since we can pinpoint
        with greater accuracy the exact data we want to load into our cache. It can allow us to
        optimize writes by enabling us to store these chunks in the same nodes as the sensors.
      </p>

      <p>
        Finally, it can optimize deletes as well - since these databases typically also maintain an
        LSM Tree and SSTable, the traditional way to delete would be to write a tombstone to our LSM
        Tree and propagate that to disk. That means that deleting a bunch of data would incur the
        same cost as writing a bunch of data. With our chunk table, we don’t need to do that - the
        organization of the data makes it easy to expire certain time interval chunks in disk by
        designating them as free memory that can just be overwritten.
      </p>

      <h3 id="time-series-databases-in-the-wild">Time Series Databases in the Wild</h3>

      <ul>
        <li>
          <a href="https://www.timescale.com/">TimescaleDB</a> - an open source, relational time
          series database
        </li>
        <li>
          <a href="https://www.influxdata.com/">InfluxDB</a> - an open source time series database
        </li>
        <li>
          <a href="https://druid.apache.org/">Apache Druid</a> - a column oriented, open source
          distributed data store primarily used for Online Analytical Processing (OLAP) use cases
        </li>
      </ul>

      <h2 id="graph-databases">Graph Databases</h2>

      <p>
        As the name implies, graph databases are used for representing graph data, or data that can
        be represented with vertices/nodes connected by edges
      </p>

      <p>
        For instance, a social graph where the vertices are people and the edges represent some kind
        of relationship (friends with, following, is followed by, etc.)
      </p>

      <h3 id="non-native-implementation">Non-Native Implementation</h3>

      <p>
        Non-native graph databases take existing database implementations (like a relational
        database, for example), and provide a query language on top of it for graph traversals
      </p>

      <p>
        In a relational database implementation, we’d have one table for vertices and another for
        edges
      </p>

      <ul>
        <li>
          Doing a graph traversal like querying all the neighbors of a given vertex would involve
          two separate binary searches on both the vertices and edges tables (assuming we have an
          index on both tables) - The time it takes to perform this query will grow as we add more
          vertices, so this is not ideal
        </li>
      </ul>

      <p>
        In a non-relational database implementation, we could get rid of the edges table and just
        store the edges alongside with the vertex for better data locality
      </p>

      <ul>
        <li>
          To be more specific, a vertex would be represented as a JSON object with an
          <code class="language-plaintext highlighter-rouge">id</code> and an
          <code class="language-plaintext highlighter-rouge">edges</code> key, which stores an array
          of all the vertex ids that are adjacent to this vertex
        </li>
        <li>
          However, this would still require us to search the vertex table with all the vertex ids in
          the <code class="language-plaintext highlighter-rouge">edges</code> array
        </li>
        <li>
          Storing the full vertex in the
          <code class="language-plaintext highlighter-rouge">edges</code> array is not viable since
          we’d have to store the neighboring vertices of <em>those</em> vertices in turn, and so on,
          creating a nested JSON abomination
        </li>
      </ul>

      <h3 id="native-implementation">Native Implementation</h3>

      <p>
        Native graph databases, in contrast, are structured specifically to accomodate graph data
      </p>

      <p>
        In Neo4J’s implementation, we again have a vertex and edges table. However, this time each
        vertex row contains its own memory address, as well as the memory address of one of its
        edges
      </p>

      <p>
        In turn, the edges table will have its own memory address, as well as the memory address of
        the vertex it points to. For a vertex that has multiple edges, the edge will also have the
        memory address of the <em>next</em> edge
      </p>

      <p>
        It’s akin to a linked list structure, where a vertex node will point to an edge node, and an
        edge node can point to another vertex node and another edge node
      </p>

      <p>
        In the end, this means we don’t have to perform binary searches on the vertices table
        whenever we want to find adjacent nodes. We can just follow the pointers and do it in
        constant time
      </p>

      <p>
        <strong>Caveat</strong>: Random I/O on disk isn’t great, but the time complexity savings
        compensate for this
      </p>

      <h2 id="geospatial-indexes">GeoSpatial Indexes</h2>

      <p>
        Geospatial indexes specifically solve the problem of finding all points within some
        distance. For example, Doordash showing you the all the restaurants closest to you, or
        within some mile radius
      </p>

      <p>Geohashes and quadtrees are the two key concepts that comprise a geospatial index.</p>

      <p>
        A <strong>quadtree</strong> is a geographic plane split into 4 subplanes. These subplanes
        can then be recursively split into sub-quadtrees. A <strong>geohash</strong> is an address
        telling us which “leaf” subplane a particular point belongs to
      </p>

      <ul>
        <li>
          For example, if we have a plane broken into 4 subplanes A, B, C, D, and each subplane is
          recursively broken into sub-subplanes A, B, C, D, a geohash for a point could be “AB”,
          telling us the point is stored a plane A, subplane B
        </li>
        <li>
          After determining the geohash of a point, we store these in sorted order (index) so that
          we can binary search them
        </li>
      </ul>

      <p>
        The neat part is that the structure of our quadtrees makes it so that points that are
        lexicographically close to one another are also <em>geographically</em> close. Therefore, we
        can also use geohashes to partition data geographically, which is known as
        <em>geosharding</em>.
      </p>

      <h2 id="object-stores">Object Stores</h2>

      <p>
        Big Data systems like Hadoop aren’t great for storing static content because we’re not
        planning to run any compute over it, we generally just want to store and retrieve it later.
        In other words, we’re wasting CPU resources when we want more disk space
      </p>

      <p>
        An <strong>object store</strong> is a service offered by a cloud provider to store static
        content. They typically are managed services that handle scaling and replication and are
        schemaless; they’re able to store any arbitrary file types
      </p>

      <p>
        Data lakes, which are centralized repositories designed to store, process, and secure large
        amounts of structured, semi-structured, and unstructured data, are built on top of object
        stores since they typically stores data in its native format (blobs and files). Performing
        batch processing over this data would require us to export it to Hadoop for batch
        processing, which can be slow.
      </p>

      <h2 id="additional-reading--material-9">Additional Reading / Material</h2>

      <ul>
        <li>
          <a href="https://www.youtube.com/watch?v=LUjVQVl4s34&amp;ab_channel=DataSharkAcademy"
            >3.4 ElasticSearch Training - Inverted Index Explained</a
          >
        </li>
        <li>
          <strong>jordanhasnolife System Design 2.0 Playlist</strong>:
          <ul>
            <li>
              <a href="https://www.youtube.com/watch?v=ty9DQhM32mM&amp;ab_channel=Jordanhasnolife"
                >“Search Indexes”</a
              >
            </li>
            <li>
              <a
                href="https://www.youtube.com/watch?v=fUpYLwzGtW0&amp;list=PLjTveVh7FakLdTmm42TMxbN8PvVn5g4KJ&amp;index=48&amp;t=206s&amp;pp=iAQB"
                >“How are Time Series Databases SO FAST?”</a
              >
            </li>
            <li>
              <a
                href="https://www.youtube.com/watch?v=Sdw_D-Gllac&amp;list=PLjTveVh7FakLdTmm42TMxbN8PvVn5g4KJ&amp;index=49&amp;t=109s&amp;pp=iAQB"
                >“How are Graph Databases So Fast?”</a
              >
            </li>
            <li>
              <a
                href="https://www.youtube.com/watch?v=9BewOp5Gaw8&amp;list=PLjTveVh7FakLdTmm42TMxbN8PvVn5g4KJ&amp;index=50"
                >“GeoSpatial Indexes - Why You Need Them”</a
              >
            </li>
          </ul>
        </li>
      </ul>

      <h1 id="networking">Networking</h1>

      <p>
        In this section we’ll take a look at some key concepts around computer networking, such as
        the protocols and mechanisms for transferring data between systems over a network or
        securing communications between client and server.
      </p>

      <h2 id="osi-model">OSI Model</h2>

      <p>
        The OSI model is a logical and conceptual framework that divides network communications
        functions into seven layers. If you’ve ever heard the terms “L7” or “L4” with regards to
        networking, this is what that’s referring to.
      </p>

      <p>
        It encapsulates every type of network communication across both software and hardware
        components and ensures that two standalone systems can communicate via standardizd
        interfaces or protocols based on the current layer of operation.
      </p>

      <p>The seven layers are as following:</p>

      <ol>
        <li>
          <strong>Physical</strong> - the <em>physical</em> cables and devices that deal with data
          transfer
        </li>
        <li>
          <strong>Data Link</strong> - the technologies used to connect two machines across a
          network where the physical layer exists (Ethernet, MAC)
        </li>
        <li>
          <strong>Network</strong> - concerned with routing, forwarding, and addressing across a
          dispersed network or multiple connected networks (IPv4, IPv6)
        </li>
        <li>
          <strong>Transport</strong> - focuses on ensuring data packets arrive in the right order
          without losses or errors and can recover seamlessly (TCP, UDP)
        </li>
        <li>
          <strong>Session</strong> - responsible for network coordination between two separate
          applications in a system (Network File System, or NFS, and Server Message Block, or SMB)
        </li>
        <li>
          <strong>Presentation</strong> - Concerned with the syntax of the data itself for
          applications to send and consume. (HTML, JSON)
        </li>
        <li>
          <strong>Application</strong> - Concerned with the specific type of application itself and
          its standardized communication methods (HTTPS, SMTP)
        </li>
      </ol>

      <h2 id="tcp-transmission-control-protocol">TCP: Transmission Control Protocol</h2>

      <p>
        TCP, or Transmission Control Protocol, is a network protocol which provides reliable 2-way
        1:1 communication through the use of handshakes and acknowledgements. TCP is used for most
        applications where reliability is crucial.
      </p>

      <p>The process for establishing a connection is via a 3-way handshake:</p>

      <ol>
        <li>Send a SYN with a sequence number x</li>
        <li>Respond with an ACK with a sequence number x + 1 and a SYN with sequence number y</li>
        <li>Respond with ACK with a sequence number y + 1</li>
      </ol>

      <p>
        TCP tries to guarantee reliable delivery using sequence numbers to confirm message receipt.
        This operates similarly to the handshake, send a message with a sequence number and expect
        an acknowledgment with the sequence number + 1
      </p>

      <p>
        TCP also uses timeouts to retry whenever it doesn’t receive an acknowledgement. For example,
        it sends a message with a sequence number, then waits for a set period before retrying the
        message. The amount of time it waits between retries increases exponentially; this is a
        strategy known as <em>exponential backoff</em>.
      </p>

      <h3 id="flow-and-congestion-control">Flow and Congestion control</h3>

      <p>
        <strong>Flow control</strong> deals with ensuring that the sender isn’t sending more
        messages than a receiver can consume. TCP does this by looking at the receiver buffer to
        limit the number of messages in flight. It then resets the limit upon receiving an
        acknowledgment.
      </p>

      <p>
        <strong>Congestion control</strong> deals with ensuring that message traffic does not
        degrade network response times. TCP handles this by setting a window of how many messages
        can be sent at once. It then uses <em>additive increase, multiplicative decrease</em> (AIMD)
        to adjust the window size. AIMD dictates that the window should grow in size linearly when
        there’s no congestion, but reduce in size exponentially when there is.
      </p>

      <h2 id="udp-user-datagram-protocol">UDP: User Datagram Protocol</h2>

      <p>
        UDP, or User Datagram Protocol, is a simple, connectionless communication model without any
        of the reliability mechanisms that TCP provides. It’s fairly bare bones - you just send
        packets from one node to another with some checksums for data integrity.
      </p>

      <p>
        For that reason, it’s super fast, and is used for low-latency scenarios where dropped data
        is acceptable. (e.g. video calls, video games, real-time tracking of stock prices). It also
        supports <strong>multicast</strong>, which is when a single packet is routed to multiple
        places.
      </p>

      <h3 id="checksums">Checksums</h3>

      <p>
        <strong>Checksums</strong> are sent with every UDP header to ensure that packets aren’t
        corrupted. Before sending off the segment, the sender:
      </p>

      <ol>
        <li>Computes the checksum based on the data in the segment</li>
        <li>Stores the computed checksum in the final two bytes of the UDP header</li>
      </ol>

      <p>Upon receiving the data, the recipient</p>

      <ol>
        <li>Computes the checksum based on the received segment</li>
        <li>
          Compares it to the checksum in the header it received. If they aren’t equal, it knows data
          was corrupted.
        </li>
      </ol>

      <h2 id="long-polling">Long Polling</h2>

      <p>
        In traditional HTTP communication, the client sends requests to the server and waits for a
        response. Once received, the client needs to make a brand new requests to receive new data.
        This is “short polling”, which might not be efficient for real time scenarios since we’d
        need to keep sending requests to the server.
      </p>

      <p>
        Long Polling, on the other hand, keeps the requests open until new data is available. Once
        it receives that data, it completes the request and creates a new one. Since creating new
        connections is expensive, long polling is best used for situations where data is rarely
        needed. In addition, it’s <strong>unidirectional</strong>, since data is only sent from the
        server to the client.
      </p>

      <h3 id="websockets">Websockets</h3>

      <p>
        <strong>WebSocket</strong> is a communications protocol which provides persistent two-way
        communication between client and server. It’s implemented over a single TCP connection,
        utilizes a special handshake incorporating the <em>HTTP Upgrade Header</em> to change from
        the HTTP protocol to WebSocket protocol, and has its own custom URI scheme: (<code
          class="language-plaintext highlighter-rouge"
          >ws://</code
        >
        for WebSocket) and (<code class="language-plaintext highlighter-rouge">wss://</code> for
        Websocket Secure).
      </p>

      <p>
        Since it’s <em>persistent</em>, we avoid the overhead of creating new connections and
        dealing with request headers. However, if these connections are largely inactive, we might
        waste resources. In addition, if a connection goes down, the client will need to manually
        re-establish it.
      </p>

      <h2 id="server-sent-events">Server-Sent Events</h2>

      <p>
        <strong>Server-Sent Events</strong> provide a persistent, unidirectional communication
        channel that reconnects automatically.
      </p>

      <p>
        Similar to Long Polling, communication is unidrectional from server to client. But then like
        with Websockets, we avoid the overhead of dealing with headers / time spent establishing
        connections since we create persistent connections. Of course, that also means we need to
        ensure we don’t maintain too many inactive connections and waste resources
      </p>

      <p>
        Automatic connection re-establishment is a convenient feature, however it might not
        necessarily be good. The <strong>thundering herd problem</strong> could occurr if the server
        goes down. When it comes back online it will try to re-establish connections with all the
        clients <em>at once</em>, causing a huge strain and potentially knocking it out again
      </p>

      <p>
        A way to mitigate this is to use some random jitter similar to the leader candidate proposal
        process in the
        <a href="/topic/06_consistency_and_consensus?subtopic=03_distributed_consensus"
          >Raft distributed consensus algorithm</a
        >.
      </p>

      <h2 id="tls-transport-layer-security">TLS: Transport Layer Security</h2>

      <p>
        TLS is a cryptographic protocol used to protect communications over a computer network. You
        probably know it as providing the “S” in “HTTPS”. TLS ensures that data sent between clients
        and servers can’t be eavesdropped on or tampered with.
      </p>

      <p>
        TLS is typically thought of as “running on top of some reliable transport protocol”, given
        that the name literally stands for “Transport layer Security”. However, it’s not a strictly
        L4 level protocol since it serves encryption to higher layers. Applications also need to
        actively initiate <em>TLS handshakes</em> and handle digital certificates.
      </p>

      <h3 id="tls-handshake">TLS Handshake</h3>

      <p>
        When a client and server agree to use TLS, they use a handshake procedure to agree on the
        keys that they’ll both use to encrypt/decrypt their data. The handshake goes like this:
      </p>

      <ol>
        <li>
          <p>
            A client connects to a TLS enabled server and presents a list of supported
            <em>cipher suites</em> and hash functions. These include algorithms for exchanging keys,
            encrypting data, and verifying data integrity.
          </p>
        </li>
        <li>
          <p>The server picks a cipher and hash function and notifies the client</p>
        </li>
        <li>
          <p>
            The server then provides identification in the form of a digital certificate. The
            certificate contains the server name, trusted certificate authority that vouches for its
            authenticity, and a public encryption key.
          </p>
        </li>
        <li>
          <p>The client confirms the validity of the certificate</p>
        </li>
        <li>
          <p>The client and server now generate session keys in one of two ways:</p>

          <p>
            a. The client encrypts a random number with the server’s public encryption key, which
            the server then decrypts with its private key. Both then use this random number to
            generate a unique session key for subsequent encryption/decryption.
          </p>

          <p>
            b. The client and server use the <a href="">Diffie-Helman</a> key exchange algorithm to
            securely generate a random and unique session key. This gives us the nice benefit of
            ensuring that the session cannot be decrypted even if the private key leaks on the
            server, which is known as <em>forward secrecy</em>
          </p>
        </li>
      </ol>

      <h3 id="tls-termination">TLS Termination</h3>

      <p>
        Performing this key exchange and subsequent encryption/decryption of data can be CPU
        intensive on our application servers. A common pattern to mitigate this is to
        <em>terminate</em> TLS using a proxy server (like a load balancer), and use regular HTTP
        when communicating between servers internally in our system.
      </p>

      <p>
        Doing this comes with all the added complexity of managing proxy load-balancer servers -
        we’d want to do this in a fault-tolerant way and without bottlenecking our system, which we
        talk about a bit more in the <a href="/topic/12_load_balancing">load balancers</a> section.
        Furthermore, if the proxy is compromised, then all our data throughout our system is
        available since it’s unencrypted.
      </p>

      <h2 id="dns-the-domain-name-system">DNS: The Domain Name System</h2>

      <p>
        The Domain Name System is responsible for turning domain names (like www.google.com) into IP
        addresses that computers can understand. It has two main components: DNS resolvers and DNS
        name servers, which work in a hierarchical, distributed fashion to resolve queries in a
        robust and performant manner.
      </p>

      <p>Let’s take a deeper look:</p>

      <h3 id="dns-resolvers-and-nameservers">DNS Resolvers and Nameservers</h3>

      <p>
        The <strong>DNS resolver</strong> is a server that’s responsible for sending and
        coordinating further downstream requests to DNS nameservers. Resolvers are maintained by
        ISPs and popular DNS providers like Google (8.8.8.8) and CloudFlare (1.1.1.1).
      </p>

      <p>
        <strong>DNS Nameservers</strong> are servers that are responsible for locating the domain’s
        corresponding IP in a hierarchical fashion. There are 3 types of nameservers
      </p>

      <ol>
        <li>
          <p>
            Root Nameservers: the first server that the DNS resolver reaches out to, which mainly
            just stores the IP addresses of the TLD nameservers.
          </p>
        </li>
        <li>
          <p>
            TLD (Top Level Domain) Nameserver: the next level of our nameserver hierarchy, which
            typically hosts the last portion of a given domain (.com, .net, .org) and stores the IP
            addresses of the respective authoritative nameservers.
          </p>
        </li>
        <li>
          <p>
            Authoritative Nameserver: the final level of the hierarchy which provides an
            authoritative answer for a given domain query. This is where the full domain name
            mapping record is stored (e.g. google.com, amazon.com).
          </p>
        </li>
      </ol>

      <h3 id="how-a-domain-gets-resolved">How A Domain Gets Resolved</h3>

      <p>Let’s take a look at an example where a user types “www.google.com” into their browser.</p>

      <ol>
        <li>
          <p>
            First, the browser checks its cache. If the result isn’t there, it makes an operating
            system call which checks <em>its</em> cache and reaches out to the DNS resolver.
          </p>
        </li>
        <li>
          <p>
            If the result isn’t cached at the DNS resolver level, the resolver reaches out to the
            Root Nameserver, which responds with a list of “.com” TLD nameserver IP addresses.
          </p>
        </li>
        <li>
          <p>
            The DNS Resolver then reaches out to one of these “.com” TLD nameservers, which returns
            the IP address for the “google.com” authoritative nameserver.
          </p>
        </li>
        <li>
          <p>
            Finally, the DNS Resolver gets the IP address for “google.com” from the authoritative
            nameserver and returns the result to the OS, which can then return that result to the
            browser.
          </p>
        </li>
      </ol>

      <h3 id="dns-propagation">DNS Propagation</h3>

      <p>
        When we register a new domain or change a domain record, we need to wait for it to propagate
        across all the DNS nameservers. This can be slow since nameservers cache domain record
        information for a certain amount of time (TTL) before refreshing. Since these TTLs are
        configured on the records themselves, one way we can speed up DNS propagation is to decrease
        the TTL value one day before implementing a change.
      </p>

      <p>
        There are various online tools for checking if DNS record changes have propagated globally,
        such as <a href="https://www.gdnspc.com/">Global DNS Checker</a> or Google’s
        <a href="https://dns.google/">DNS Checker</a>.
      </p>

      <h2 id="additional-reading--material-10">Additional Reading / Material</h2>

      <ul>
        <li>
          KhanAcademy
          <a
            href="https://www.khanacademy.org/computing/computers-and-internet/xcae6f4a7ff015e7d:the-internet/xcae6f4a7ff015e7d:transporting-packets/a/user-datagram-protocol-udp"
            >“Computers and the Internet: User Datagram Protocol”</a
          >
        </li>
        <li>
          PubNub <a href="https://www.pubnub.com/guides/long-polling/">“What is Long Polling?”</a>
        </li>
        <li>
          AWS Cloud Computing Concepts Hub
          <a href="https://aws.amazon.com/what-is/osi-model/">“What is OSI Model?”</a>
        </li>
        <li>
          <strong>jordanhasnolife System Design 2.0 Playlist</strong>:
          <ul>
            <li>
              <a
                href="https://www.youtube.com/watch?v=hPSsPCNxta4&amp;list=PLjTveVh7FakLdTmm42TMxbN8PvVn5g4KJ&amp;index=58"
                >“TCP vs. UDP in 12 minutes”</a
              >
            </li>
            <li>
              <a
                href="https://www.youtube.com/watch?v=fIwOd4PToAY&amp;list=PLjTveVh7FakLdTmm42TMxbN8PvVn5g4KJ&amp;index=59"
                >“Long Polling, Websockets, Server-Sent Events”</a
              >
            </li>
          </ul>
        </li>
        <li>
          <strong>Hussein Nasser</strong>:
          <a href="https://www.youtube.com/watch?v=H0bkLsUe3no"
            >“SSL/TLS Termination, TLS Forward Proxy Pros and Cons”</a
          >
        </li>
        <li>
          <strong>ByteByteGo</strong>:
          <a href="https://www.youtube.com/watch?v=27r4Bzuj5NQ"
            >“Everything You Need To Know About DNS”</a
          >
        </li>
        <li>
          <table>
            <tbody>
              <tr>
                <td>CloudFlare Learning Center: [“What is DNS</td>
                <td>How DNS works”](https://www.cloudflare.com/learning/dns/what-is-dns/)</td>
              </tr>
            </tbody>
          </table>
        </li>
      </ul>

      <h1 id="load-balancing">Load Balancing</h1>

      <p>
        Load balancing is the method of distributing network traffic equally across a pool of
        resources that support an application. It’s very important in a distributed system to load
        balance intelligently so we don’t overwhelm any of our key components whenever we have large
        amounts of traffic coming through.
      </p>

      <h2 id="horizontal-vs-vertical-scaling">Horizontal vs. Vertical Scaling</h2>

      <p>First, let’s look at the different ways we can scale a system</p>

      <p>
        <strong>Vertical scaling</strong> means improving the performance of our system by upgrading
        the hardware of our application server or database node.
      </p>

      <p>
        <strong>Horizontal scaling</strong> means improving performance by distributing
        computational load across more machines, which are usually commodity hardware. This is
        typically where load balancing comes into play
      </p>

      <h2 id="routing-policies">Routing Policies</h2>

      <p>There are several ways to route traffic in our load balancers:</p>

      <p><strong>Weighted or Unweighted Round Robin:</strong></p>

      <p>
        A round robin policy is when we distribute requests to each server sequentially and wrap
        around. (you can imagine it’s like dealing cards in Poker to each player).
      </p>

      <p>
        An <em>unweighted</em> round robin strategy just means we naively distribute traffic equally
        across all our nodes. In constrast, a <em>weighted</em> strategy sends more requests to a
        given node if it has a greater weight.
      </p>

      <p><strong>Lowest Response Time:</strong></p>

      <p>
        In a lowest response time routing strategy, we keep a running average of the response times
        of each node. We then route requests to the ones with the lowest response time.
      </p>

      <p><strong>Hashing:</strong></p>

      <p>
        Hashing is where we take a hash of request content or headers and distribute load to each
        node corresponding to the hash value. We can do this at the L4 or L7 layer. (Recall in the
        OSI model of networking, L4 corresponds to the Transport Layer, like TCP, and L7 corresponds
        to the application layer, like HTTP)
      </p>

      <ul>
        <li>
          <strong>L4:</strong> Hash information based on information exposed at the networking
          layer, like IP, protocol, etc. (faster).
        </li>
        <li>
          <strong>L7:</strong> Hash information available in the actual message content (slower but
          more flexible).
        </li>
      </ul>

      <p>
        It’s important that we use consistent hashing so that every request goes to the same
        partition.
      </p>

      <h2 id="fault-tolerance-1">Fault Tolerance</h2>

      <p>Let’s look at 2 ways we can ensure fault tolerance in our load balancer setups.</p>

      <h3 id="active-active">Active-Active</h3>

      <p>
        In an active-active setup, we have more than 1 active load balancer. That means requests
        come into both load balancers, and both load balancers actively redirect traffic to the
        appropriate downstream application servers.
      </p>

      <p><strong>Pros</strong></p>

      <ul>
        <li>Higher throughput since requests can be split across the two load balancers.</li>
        <li>Fault tolerance since we essentially have replicas of our load balancers.</li>
      </ul>

      <p><strong>Cons</strong></p>

      <ul>
        <li>
          Might make local state maintained on the load balancer (like keeping track of average
          response time per server) a bit more complicated.
        </li>
        <li>Complexity in configuration.</li>
        <li>
          Potentially higher costs since we need to run multiple load balancers at full capacity.
        </li>
      </ul>

      <h3 id="active-passive">Active-Passive</h3>

      <p>
        In an active-passive setup, we have an active load balancer that actually performs the
        request routing and one passive one that sits idly on standby. We utilize a coordinator
        service like Zookeeper, which will continuously send heartbeats to the active load balancer
        and swap over to the passive one in the event of a failure.
      </p>

      <p><strong>Pros:</strong></p>

      <ul>
        <li>Simplicity - we just have one load balancer to manage.</li>
        <li>Cost-effective - only one load balancer is run at full capacity.</li>
      </ul>

      <p><strong>Cons:</strong></p>

      <ul>
        <li>Lower throughput.</li>
        <li>Underutilization of resources in passive nodes.</li>
      </ul>

      <h2 id="additional-reading--material-11">Additional Reading / Material</h2>

      <p>
        <strong>jordanhasnolife System Design 2.0 Playlist</strong>
        <a
          href="https://www.youtube.com/watch?v=PERKHUJYotM&amp;list=PLjTveVh7FakLdTmm42TMxbN8PvVn5g4KJ&amp;index=57"
          >“Load Balancing - The Right Way to Do It”</a
        >
      </p>

      <h1 id="software-architecture">Software Architecture</h1>

      <p>
        In this section, we’ll take a look at how software systems are typically organized at scale
        and the tradeoffs associated with various paradigms.
      </p>

      <h2 id="monoliths-and-microservices">Monoliths and Microservices</h2>

      <h3 id="monolithic-architecture">Monolithic Architecture</h3>

      <p>
        A <strong>Monolithic architecture</strong> is a singular system with one code base that
        couples all business concerns together. These are convenient early on in a project’s life
        since they’re easy to understand and manage. Testing and debugging is streamlined since we
        just have a single codebase or service, and we don’t need to coordinate business logic
        across multiple endpoints which might give us some nice performance benefits.
      </p>

      <p>
        However as a system grows, some major problems begin to arise: For one thing, we have a
        single point of failure so our system is less reliable. We also can’t scale up individual
        parts of our system independently, or use different technologies for each part since it’s
        one giant codebase. Finally, making changes requires updating and deploying the entire
        system, which can be slow and inefficient.
      </p>

      <h3 id="microservices">Microservices</h3>

      <p>
        A <strong>Microservices architecture</strong> relies on coordinating a series of
        independently managed and deployed services. Each “microservice” is responsible for doing
        one thing and thus all development and testing is encapsulated. They solve for many of the
        problems we saw before with monolithic architectures in that they allow for faster, smaller
        scale updates to our system and the flexibility to choose different technologies which might
        be more suited for specific domains. They also give us higher reliability - we can isolate
        failures within our system now that logic is split across many different services.
      </p>

      <p>
        There are some downsides as well - with more services comes more management complexity and
        costs. Communication and agreement on interfaces across service owner teams can bog down
        development speed. Allowing a variety of technologies can degrade standardization and
        present challenges in logging and monitoring. Furthermore, with each new service comes its
        own hosting infrastructure and tooling, which could mean exponentially increasing costs as
        the system scales.
      </p>

      <p>
        Microservices architectures are widely adopted at large companies operating at high scale,
        so the next few sections will explore some methods of developing and maintaining them.
      </p>

      <h2 id="containers">Containers</h2>

      <p>
        A <strong>container</strong> is a standard unit of software that packages up code and all
        its dependencies for reliable, efficient execution and deployment. In the context of
        microservices, each service might be using different technologies and runtimes. Containers
        allow us to bundle these up in a nice, scalable way.
      </p>

      <p>
        A widely used platform for building and maintaing software containers is
        <a href="https://www.docker.com/">Docker</a>
      </p>

      <h3 id="virtual-machines-vs-containers">Virtual Machines v.s Containers</h3>

      <p>
        <strong>Virtual Machines</strong> are instances of operating systems co-located on the same
        physical machine. These are coordinated through the use of a hypervisor, which abstracts the
        physical host hardware from the operating system environment. Each Virtual Machine has its
        own OS, memory and other resources, which are isolated from other VMs.
      </p>

      <p>
        <strong>Containers</strong>, on the other hand, are lightweight, portable, executable images
        that only concern themselves with application software and their dependencies. Unlike VMs,
        they share the same operating system kernel so they use fewer resources and can start and
        stop faster. Containers also allow us to dynamically scale resources, unlike VMs, where we
        need to specify our resource requirements in advance.
      </p>

      <p>
        Virtual Machines and containers aren’t mutually exclusive. A container could run within a VM
        to reap the benefits of both worlds, namely the portability and speed afforded by containers
        and the security afforded by isolated VM environments.
      </p>

      <h3 id="kubernetes--container-management-systems">
        Kubernetes &amp; Container Management Systems
      </h3>

      <p>
        A few problems arise when we start using a whole bunch of containers together in our
        microservices architecture. How do we know which host to put each container on? And what
        should we do in the event of a host failure?
      </p>

      <p>
        Kubernetes and other container management systems provide a “control plane” to deal with
        these kinds of issues. In Kubernetes specifically, we have a control plane node that runs
        some kind of coordination service like Zookeeper or Etcd. It talks to all of our hosts
        through “Kubelets”, which are agents that run on each host. These Kubelets coordinate and
        provision “pods”, which correspond to individual Docker containers. If a container fails,
        the Kubelet can restart it, and if a host goes down, the control plane can replicate it over
        to another host.
      </p>

      <h2 id="elk---elasticsearch-logstash-and-kibana">ELK - Elasticsearch, Logstash and Kibana</h2>

      <p>
        In previous sections, we looked a the microservices architecture and the advantages it gives
        us in terms of independent scalability, flexibility, and reliability in a large software
        system. If you’ll recall, one of the drawbacks was the inability to enforce standardized
        monitoring and logging across these hundreds or potentially thousands of services.
      </p>

      <p>
        Enter the <strong>ELK stack</strong> - an acronym used to describe a technology stack
        comprised of three popular technologies, Elasticsearch, Logstash, and Kibana. The ELK stack
        aims to provide an easy way to aggregate, analyze and vizualize logs across many different
        services, and has become very popular for troubleshooting and monitoring large
        microservices-based systems.
      </p>

      <p>Let’s take a look at each component in further detail</p>

      <h3 id="elasticsearch-1">Elasticsearch</h3>

      <p>
        We briefly mentioned Elasticsearch we we talked about
        <a href="/topic/10_specialized_data_stores_indexes?subtopic=01_search_indexes"
          >search indexes</a
        >. As a refresher, it’s a distributed search index built on top of Apache Lucene, and it
        provides some nice features for analytics and querying. The key thing to note in the context
        of the ELK stack is that often times, Elasticsearch is used more as a datastore for logs
        than as a search index
      </p>

      <h3 id="logstash">Logstash</h3>

      <p>
        Logstash is an open source data-ingestion tool that transforms and aggregates data from
        multiple sources to be republished elswhere. It functions as the entrypoint for your
        microservices to send their logs to. Traditionally it was used to just parse and forward log
        data to Elasticsearch to be analyzed. In more recent years, Logstash has started to become a
        more general-purpose <a href="/topic/09_stream_processing">stream processing consumer</a>,
        and has a rich plugin ecosystem that lets you customize how you ingest, parse, and publish
        your data, with support for multiple downstream destinations (which it refers to as
        “stashes”).
      </p>

      <h3 id="kibana">Kibana</h3>

      <p>
        Kibana is an analytics and visualization platform which serves as the view layer for the ELK
        stack. It provides rich functionality for creating various realtime data visualizations like
        charts, maps and tables. It’s basically a web platform that queries data from Elasticsearch
        via its REST API and provides configurations for how to display the results.
      </p>

      <h3 id="from-elk-to-elastic-stack">From ELK to Elastic Stack</h3>

      <p>
        Today, the ELK Stack has evolved to incorporate two new components (X-Pack and Beats) and is
        now more commonly referred to as the “Elastic stack”. X-Pack is a pack of features which
        provide additional functionality on top of ElasticSearch, such as graph visualizations and
        machine learning based anomaly detection. Beats is an event publishing agent that can be
        installed on your microservice hosts to standardize how you publish your log files
        (FileBeat) and metrics (MetricBeat).
      </p>

      <p>
        <strong>Note:</strong> The ELK stack started off as a fully open source project, but has
        recently become a proprietary, managed offering provided by the Elastic NV company after
        transitioning from the Apache License v2 to the dual Server Side Public License and Elastic
        License. AWS provides alternatives to Elasticsearch and Kibana under the old Apache License
        in the form of OpenSearch and OpenSearch Dashboards.
      </p>

      <h2 id="microservices-and-fault-tolerance">Microservices and Fault Tolerance</h2>

      <p>
        Fault tolerance becomes a non-trivial problem when we have hundreds of microservices
        interacting with each other. What happens when a critical service with lots of upstream
        dependencies fails? A typical way to recover from failures is to <em>retry</em> the
        requests, and exponentially back off (similar to the way
        <a href="/topic/11_networking?subtopic=02_tcp">TCP performs congestion control</a>).
        However, if the service is down for an extended period of time, we could end up with retried
        requests piling up in these upstream dependencies, leading to resource exhaustion and
        <em>cascading failures</em>.
      </p>

      <h3 id="circuit-breakers">Circuit Breakers</h3>

      <p>
        The <strong>circuit breaker</strong> patern aims to solve the problem of
        <em>cascading failures</em> as a result of service downtime. As the name implies, the
        circuit breaker pattern is based on electrical circuit breakers, which automatically
        interrupt current flow during power surges or faults to protect cascading failures in your
        electronic devices and appliances.
      </p>

      <h4 id="circuit-breaker-states">Circuit Breaker States</h4>

      <p>
        Software circuit breakers are practically implemented as request interceptors and have 3
        states: open, half open, and closed:
      </p>

      <ul>
        <li>
          When the circuit breaker is <strong>closed</strong>, it allows requests to execute
          normally. While doing so, it measures faults and successes, and if the number of faults
          exceed a certain threshold, the circuit will break and enter the open state.
        </li>
        <li>
          When the circuit breaker is <strong>open</strong>, it prevents the execution of all
          requests passing through it. The circuit breaker will remain in this state for some
          configured timespan, at which point it will enter the half-open state.
        </li>
        <li>
          Finally, when the circuit breaker is <strong>half-open</strong>, it will retry
          <em>some</em> but not all of the failed requests. If these requests continue to fail, the
          circuit breaker will re-enter the open state and remain that way for another break period.
          Otherwise, it will close and start handling requests normally.
        </li>
      </ul>

      <p>
        One additional optimization we can make is to configure a <em>fallback</em> policy. When a
        downstream API fails, we can specify an alternative backup service or cache that the circuit
        breaker can redirect requests to. This will give us higher availability in our overall
        system, as we can continue to satisfy client queries when a service breaks.
      </p>

      <h3 id="rate-limiting">Rate Limiting</h3>

      <p>
        Another way to provide fault tolerance is to pre-emptively
        <strong>rate-limit</strong> upstream dependencies from flooding our service with requests.
        This is especially important when we’re dealing with malicious threats to fault tolerance
        external to our system, such as Distributed Denial of Service (DDOS) attacks.
      </p>

      <p>There are a few ways of rate-limiting and API throttling in software systems:</p>

      <ol>
        <li>
          <strong>Fixed Window</strong>: Specifiy a limit on how many requests a client can send in
          a fixed window of time. After the fixed window passes, that limit resets.
        </li>
        <li>
          <p>
            <strong>Token Bucket</strong>: We specify a “bucket” with a capacity to hold N tokens.
            Whenever a request comes in, it “redeems” a token and is fulfilled. When all the tokens
            are gone, requests will be dropped / fail. The bucket will refill tokens at some
            speficied maximum refill rate.
          </p>

          <ul>
            <li>
              The maximum refill rate in this scenario is the maximum rate that users will have on
              average.
            </li>
            <li>
              The max capacity can be <strong>larger</strong> than the max refill rate. That means
              that for a given point in time, we could have a large number of requests coming in at
              once which consumes all available tokens (capacity). However subsequent requests will
              need to wait for the tokens to refill. Thus we are able to accommodate bursty traffic
              while still guaranteeing a maximum requests per second rate.
            </li>
          </ul>
        </li>
      </ol>

      <h3 id="microservice-fault-tolerance-in-the-wild">
        Microservice Fault Tolerance in the Wild
      </h3>

      <ul>
        <li>
          <a href="https://github.com/Netflix/Hystrix">Hystrix</a> was a latency and fault tolerance
          library designed by Netflix to isolate points of access to remote systems, services and
          3rd party libraries. It entered maintenance mode in 2020.
        </li>
        <li>
          <a href="https://resilience4j.readme.io/docs/getting-started">Resilience4J</a> has been
          the replacement for Hystrix ever since, and is a lightweight fault tolerance library with
          a functional programming model for Java applications. It implements the circuit breaker,
          rate-limiter, and retry patterns
        </li>
      </ul>

      <h2 id="serverless-computing">Serverless Computing</h2>

      <p>
        Serverless computing is a method of providing compute resources on an as-used basis,
        enabling users to write and deploy code without needing to worry about the underlying
        infrastructure. The concept was first pioneered by Amazon in 2014 with the introduction of
        AWS Lambda.
      </p>

      <p>
        The term serverless is somewhat of a misnomer, as servers are still being used behind the
        scenes. The main selling point is that the actual configuration and provisioning of those
        servers is entirely handled by the vendor, effectively hiding them from the developer.
      </p>

      <p>Some examples of serverless providers (other than AWS Lambda) are:</p>

      <ul>
        <li>
          <a href="https://cloud.google.com/functions#key-features">Google Cloud Functions</a>
        </li>
        <li>
          <a href="https://azure.microsoft.com/en-us/solutions/serverless"
            >Microsoft Azure Serverless</a
          >
        </li>
        <li><a href="https://www.serverless.com/">Serverless Framework</a></li>
      </ul>

      <h3 id="advantages--disadvantages">Advantages &amp; Disadvantages</h3>

      <p>Serverless computing has several advantages:</p>

      <ul>
        <li>
          <strong>Potentially lower cost</strong>: If services have uneven traffic load, serverless
          may be a more cost-effective solution since we don’t waste resources and money running
          idle servers
        </li>
        <li>
          <strong>Simplified scalability</strong>: Vendors automatically scale resource allocation
          based on usage, vastly simplifying the process for developers.
        </li>
        <li>
          <strong>Simplified code</strong>: Typically the developer just writes a function rather
          than needing to setup all the boilerplate for running a server.
        </li>
      </ul>

      <p>However, it has some downsides:</p>

      <ul>
        <li>
          <strong>Cold Start Problem</strong>: When serverless functions aren’t invoked for a long
          time, vendors will dynamically scale down resource usage to zero. Subsequent invocations
          to this “cold” function will require all of these resources to be spun back up, which
          could result in high latency
          <ul>
            <li>
              There are some ways of mitigating this, such as a minimum allocation setting to keep
              functions “warm”
            </li>
          </ul>
        </li>
        <li>
          <p>
            <strong>Costly for multi-step workflows or long-running processes</strong>: Long running
            processes with consistent load are typically better off using traditional servers.
            Reserved hardware is typically much more cost effective in these scenarios.
          </p>

          <p>
            Furthermore, multi-step workflows can be extremely inefficient and expensive. A famous
            example is when
            <a
              href="https://www.primevideotech.com/video-streaming/scaling-up-the-prime-video-audio-video-monitoring-service-and-reducing-costs-by-90"
              >Amazon Prime Video cut costs by 90%</a
            >
            in 2023 after switching from a serverless architecture to a monolithic one for their
            video-quality inspection workflow.
          </p>
        </li>
      </ul>

      <h2 id="additional-reading--material-12">Additional Reading / Material</h2>

      <ul>
        <li>
          Atlassian Microservices Guide:
          <a
            href="https://www.atlassian.com/microservices/microservices-architecture/microservices-vs-monolith#:~:text=A%20monolithic%20architecture%20is%20a%20singular%2C%20large%20computing%20network%20with,of%20the%20service%2Dside%20interface."
            >“Monolithic vs. Microservices Architecture”</a
          >
        </li>
        <li>
          <strong>jordanhasnolife System Design 2.0</strong>:
          <a
            href="https://www.youtube.com/watch?v=kDHb99gTByU&amp;list=PLjTveVh7FakLdTmm42TMxbN8PvVn5g4KJ&amp;index=60"
            >“Monolith vs. Microservices + Docker + Kubernetes”</a
          >
        </li>
        <li>
          Google Cloud:
          <a href="https://cloud.google.com/discover/containers-vs-vms">“Containers vs. VMs”</a>
        </li>
        <li>
          <strong>Coding Explained</strong>:
          <a href="https://www.youtube.com/watch?v=Hqn5p67uev4"
            >“Overview of the Elastic Stack (formerly ELK Stack)”</a
          >
        </li>
        <li>
          AWS Cloud Computing Concepts Hub:
          <a href="https://aws.amazon.com/what-is/elk-stack/">“What is the ELK Stack?”</a>
        </li>
        <li>
          <table>
            <tbody>
              <tr>
                <td><strong>Nick Chapsas</strong>: [“The Circuit Breaker Pattern</td>
                <td>Resilient Microservices”](https://www.youtube.com/watch?v=5_Bt_OEg0no)</td>
              </tr>
            </tbody>
          </table>
        </li>
        <li>
          <table>
            <tbody>
              <tr>
                <td><strong>Be A Better Dev</strong>: [“What is Rate Limiting / API Throttling?</td>
                <td>System Design Concepts”](https://www.youtube.com/watch?v=9CIjoWPwAhU)</td>
              </tr>
            </tbody>
          </table>
        </li>
        <li>
          <strong>Cloudflare Learning Center</strong>:
          <a href="https://www.cloudflare.com/learning/serverless/what-is-serverless/"
            >“What is Serverless?”</a
          >
        </li>
      </ul>
    </div>
    <script
      src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.1.0/anchor.min.js"
      integrity="sha256-lZaRhKri35AyJSypXXs4o6OPFTbTmUoltBbDCbdzegg="
      crossorigin="anonymous"
    ></script>
    <script>
      anchors.add()
    </script>
  </body>
</html>
